{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sE6otXkDb4ZQ",
        "U421al6XcdOI",
        "5SHHkO19xC-A",
        "H0r54xTN7-z2",
        "WQ3VO_FxUVY2",
        "0SgInfZ3X4W7",
        "-dF50PwWauJZ",
        "bTZFZBItdSQv",
        "5fV4uK-Rf-Pd",
        "H1emn--_jjna",
        "i7B4KAcGvN9o"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b178a249100a4822aaa6de774e2aad6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ea247767f934ef18fefcad1bce94d55",
              "IPY_MODEL_c2a204dbbe15481486332dc5d7817a0b",
              "IPY_MODEL_23cafe617bbc49df97f9a1053861601c"
            ],
            "layout": "IPY_MODEL_f066f8850f594f06bafb723617f14b63"
          }
        },
        "9ea247767f934ef18fefcad1bce94d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c62d55b5860445b8aa2ace125b1c6676",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6c1f769d8da3472e92351012915995c0",
            "value": "Downloading‚Äáhttps://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:‚Äá"
          }
        },
        "c2a204dbbe15481486332dc5d7817a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f8c5d86b7284b30a7777a888bfcec44",
            "max": 52741,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e283bd8470bf4002a087e9938ac2fc6c",
            "value": 52741
          }
        },
        "23cafe617bbc49df97f9a1053861601c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae0d06a450214cd2b4a04ff197718a10",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_30dcb8ea5b294df58efe04a70a40fcbe",
            "value": "‚Äá426k/?‚Äá[00:00&lt;00:00,‚Äá5.70MB/s]"
          }
        },
        "f066f8850f594f06bafb723617f14b63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c62d55b5860445b8aa2ace125b1c6676": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c1f769d8da3472e92351012915995c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f8c5d86b7284b30a7777a888bfcec44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e283bd8470bf4002a087e9938ac2fc6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae0d06a450214cd2b4a04ff197718a10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30dcb8ea5b294df58efe04a70a40fcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ingrid-Garcia/Fase-2-usoNLP/blob/main/Pruebas_Unitarias_Reto_Fase_2_Parte_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pruebas Unitarias - Fase 2\n",
        "Equipo 5:<br>\n",
        "Ingrid Garc√≠a Hern√°ndez, A01754475<br>\n",
        "Abigail Donaj√≠ Chavez Rubio, A01747423<br>\n",
        "Noh Ah Kim Kwon, A01747512<br>\n",
        "Eduardo Alfredo Ram√≠rez Mu√±oz, A01754917\n"
      ],
      "metadata": {
        "id": "7ap2fwoJK5jE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Definici√≥n de librer√≠as**"
      ],
      "metadata": {
        "id": "sE6otXkDb4ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoB32rcGSOOK",
        "outputId": "6b040f92-2b0a-4738-d35a-a16215b4f1e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stanza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZtBQu87SREu",
        "outputId": "6d99f29d-d35a-4330-c0cf-545df10b77cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stanza in /usr/local/lib/python3.11/dist-packages (1.10.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from stanza) (2.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from stanza) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (5.29.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from stanza) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from stanza) (3.4.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stanza) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (4.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install --upgrade numpy==1.25.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTOl1fMZS417",
        "outputId": "eea0b6f7-d06d-4bcf-8382-5f9c920bc10a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: numpy==1.25.2 in /usr/local/lib/python3.11/dist-packages (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import brown\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.util import ngrams\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "from sklearn.metrics import pairwise\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "\n",
        "from six import StringIO\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "\n",
        "import pydotplus\n",
        "\n",
        "\n",
        "import stanza\n",
        "nlp = stanza.Pipeline('es')\n",
        "\n",
        "import string\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import unittest\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560,
          "referenced_widgets": [
            "b178a249100a4822aaa6de774e2aad6e",
            "9ea247767f934ef18fefcad1bce94d55",
            "c2a204dbbe15481486332dc5d7817a0b",
            "23cafe617bbc49df97f9a1053861601c",
            "f066f8850f594f06bafb723617f14b63",
            "c62d55b5860445b8aa2ace125b1c6676",
            "6c1f769d8da3472e92351012915995c0",
            "5f8c5d86b7284b30a7777a888bfcec44",
            "e283bd8470bf4002a087e9938ac2fc6c",
            "ae0d06a450214cd2b4a04ff197718a10",
            "30dcb8ea5b294df58efe04a70a40fcbe"
          ]
        },
        "id": "oz-OqGYQc9l6",
        "outputId": "ba04ee53-8394-4299-e0ad-244bbfa0d2a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  ‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b178a249100a4822aaa6de774e2aad6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: es (Spanish):\n",
            "====================================\n",
            "| Processor    | Package           |\n",
            "------------------------------------\n",
            "| tokenize     | combined          |\n",
            "| mwt          | combined          |\n",
            "| pos          | combined_charlm   |\n",
            "| lemma        | combined_nocharlm |\n",
            "| constituency | combined_charlm   |\n",
            "| depparse     | combined_charlm   |\n",
            "| sentiment    | tass2020_charlm   |\n",
            "| ner          | conll02           |\n",
            "====================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: constituency\n",
            "INFO:stanza:Loading: depparse\n",
            "INFO:stanza:Loading: sentiment\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prueba limpieza**"
      ],
      "metadata": {
        "id": "U421al6XcdOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def limpiar_texto(texto):\n",
        "    '''\n",
        "    La funci√≥n de limpiar_texto(texto), tiene como objetivo eliminar el ruido\n",
        "    dentro de cada elemento de la lista.\n",
        "    El dato de entrada es una lista de strings y el dato de salida es\n",
        "    una lista de strings.\n",
        "    Por cada palabra se debe de normalizar, es decir, se hace caso omiso de los\n",
        "    acentos y despu√©s se revisa cada palabra con respecto a  las reglas\n",
        "    definidas. Esto se hace por medio de expresiones regulares. Igualmente, se\n",
        "    eliminan las palabras 'innecesarias',dado que no proporcionan mayor\n",
        "    informaci√≥n o contexto.\n",
        "    '''\n",
        "    #texto.lower()\n",
        "    tweet = []\n",
        "    for palabra in texto.split():\n",
        "      nuevo = palabra\n",
        "      #Acentos\n",
        "      nuevo = unicodedata.normalize ('NFKD', nuevo).encode ('ascii', 'ignore').decode ('utf-8', 'ignore')\n",
        "      for i, j in reglas.items():\n",
        "        if i == 'espacios_extra':\n",
        "          nuevo = re.sub(j, ' ', nuevo)\n",
        "        else:\n",
        "          nuevo = re.sub(j, '',nuevo)\n",
        "      if nuevo not in stopwords.words('spanish'):\n",
        "        tweet.append(nuevo.lower())\n",
        "\n",
        "    return tweet\n",
        "\n",
        "reglas = {\n",
        "    'hashtags': r'#',\n",
        "    'menciones': r'@',\n",
        "    'urls': r'http\\S+|www.\\S+',\n",
        "    'emojis': r'[^\\x00-\\x7F]+',\n",
        "    'puntuacion': r'[^\\w\\s]',\n",
        "    'espacios_extras': r'\\s+' #Tabs, dobles espacios\n",
        "}"
      ],
      "metadata": {
        "id": "f8Gm7nofcgyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casos de prueba"
      ],
      "metadata": {
        "id": "s0WNyKFAd6kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestReglasLimpieza(unittest.TestCase):\n",
        "\n",
        "    def test_hashtags(self):\n",
        "        texto = \"Amo la #comida y mi #cuerpo\"\n",
        "        salida = limpiar_texto(texto)\n",
        "        esperado = [\"amo\", \"comida\", \"cuerpo\"]\n",
        "        self.assertEqual(salida, esperado)\n",
        "\n",
        "    def test_menciones(self):\n",
        "        texto = \"Gracias @eduardo por tu apoyo\"\n",
        "        salida = limpiar_texto(texto)\n",
        "        esperado = [\"gracias\", \"eduardo\" , \"apoyo\"]\n",
        "        self.assertEqual(salida, esperado)\n",
        "\n",
        "    def test_urls(self):\n",
        "        texto = \"Visita http://teApoyamos.com o www.nutricion.com\"\n",
        "        salida = limpiar_texto(texto)\n",
        "        esperado = ['visita', '', '']\n",
        "        self.assertEqual(salida, esperado)\n",
        "\n",
        "    def test_emojis(self):\n",
        "        texto = \"Estoy muy feliz üòÑ por este logro üëè\"\n",
        "        salida = limpiar_texto(texto)\n",
        "        esperado = ['estoy', 'feliz', '', 'logro', '']\n",
        "        self.assertEqual(salida, esperado)\n",
        "\n",
        "    def test_puntuacion(self):\n",
        "        texto = \"¬°Hola! ¬øC√≥mo est√°s? Bien, gracias.\"\n",
        "        salida = limpiar_texto(texto)\n",
        "        esperado = [\"hola\", \"como\", \"bien\", \"gracias\"]\n",
        "        self.assertEqual(salida, esperado)\n",
        "\n",
        "    def test_espacios_extras(self):\n",
        "        texto = \"Hola     mundo, esto    es   una     prueba.\"\n",
        "        salida = limpiar_texto(texto)\n",
        "        esperado = [\"hola\", \"mundo\", \"prueba\"]\n",
        "        self.assertEqual(salida, esperado)\n",
        "\n",
        "unittest.main(argv=[''], exit=False, verbosity=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsdANEVKcweJ",
        "outputId": "98813c9a-4650-45b4-9356-d6b4e4659ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
            "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
            "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
            "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
            "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
            "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 6 tests in 0.022s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7e0efdee3090>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prueba jergas**"
      ],
      "metadata": {
        "id": "5SHHkO19xC-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jerga_proceso(texto):\n",
        "  '''\n",
        "    La funci√≥n de jerga(texto), tiene como objetivo proporcionar informaci√≥n m√°s\n",
        "    precisa de palabras relaciondas con este desorden alimenticio, mas no lo\n",
        "    hacen de manera directa.\n",
        "    El dato de entrada es una lista de lista de strings y el dato de salida es\n",
        "    una lista de strings.\n",
        "    Se asuma que la entrada es una lista de palabras y por ende revisa cada una\n",
        "    de estas para revisar que no se haga caso omiso de ning√∫n caso de jerga.\n",
        "    '''\n",
        "  tweet = []\n",
        "  for palabra in texto:\n",
        "    if palabra == 'ana':\n",
        "      palabra = 'anorexia'\n",
        "    if palabra == 'anorexic':\n",
        "      palabra = 'anorexia'\n",
        "    if palabra == 'm√≠a':\n",
        "      palabra = 'bulimia'\n",
        "    if palabra == 'bulimic':\n",
        "      palabra = 'bulimia'\n",
        "    if palabra == 'mia':\n",
        "      palabra = 'bulimia'\n",
        "    if palabra == 'promia':\n",
        "      palabra = 'bulimia'\n",
        "    if palabra == 'bulymia':\n",
        "      palabra = 'bulimia'\n",
        "    if palabra == 'proana':\n",
        "      palabra = 'anorexia'\n",
        "    if palabra == 'styleana':\n",
        "      palabra = 'anorexia'\n",
        "    if palabra == 'miaana':\n",
        "      palabra = 'anorexia'\n",
        "    if palabra == 'anoreccia':\n",
        "      palabra = 'anorexia'\n",
        "    tweet.append(palabra)\n",
        "\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "DA4jzaAkxG5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casos de prueba"
      ],
      "metadata": {
        "id": "hI0UWZzs7xTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestJergaProceso(unittest.TestCase):\n",
        "\n",
        "    def test_palabras_convertidas_a_anorexia(self):\n",
        "        entrada = [\"ana\", \"anorexic\", \"proana\", \"styleana\", \"miaana\", \"anoreccia\"]\n",
        "        salida = jerga_proceso(entrada)\n",
        "        for palabra in salida:\n",
        "            self.assertEqual(palabra, \"anorexia\")\n",
        "\n",
        "    def test_palabras_convertidas_a_bulimia(self):\n",
        "        entrada = [\"mia\", \"m√≠a\", \"bulimic\", \"promia\", \"bulymia\"]\n",
        "        salida = jerga_proceso(entrada)\n",
        "        for palabra in salida:\n",
        "            self.assertEqual(palabra, \"bulimia\")\n",
        "\n",
        "    def test_palabras_no_convertidas(self):\n",
        "        entrada = [\"feliz\", \"comer\", \"vida\"]\n",
        "        salida = jerga_proceso(entrada)\n",
        "        self.assertEqual(salida, entrada)\n",
        "\n",
        "    def test_mixto_convertidos_y_normales(self):\n",
        "        entrada = [\"ana\", \"feliz\", \"mia\", \"vida\"]\n",
        "        salida = jerga_proceso(entrada)\n",
        "        self.assertEqual(salida, [\"anorexia\", \"feliz\", \"bulimia\", \"vida\"])\n",
        "\n",
        "unittest.main(argv=[''], exit=False, verbosity=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhX5UrCUz12m",
        "outputId": "93a33ac7-9b07-42a5-bfeb-7d49446331ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
            "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
            "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
            "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
            "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
            "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
            "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
            "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
            "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
            "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 10 tests in 0.032s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7e0efdee2e10>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prueba lematizaci√≥n**"
      ],
      "metadata": {
        "id": "H0r54xTN7-z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lematizacionProc(tweet):\n",
        "    '''\n",
        "    La funci√≥n lematizacionProc(tweet), tiene como objetivo lematizar cada tweet,\n",
        "    esto con el fin de obtener la forma de cada palabra.\n",
        "    El dato de entrada es una lista de strings y el dato de salida es\n",
        "    una lista de strings.\n",
        "    Dado que es un proceso de lematizaci√≥n se requiere contar con la oraci√≥n\n",
        "    completa, para despu√©s proceder a unir cada lemma encontrado en una lista.\n",
        "    '''\n",
        "\n",
        "    frase = ' '.join(tweet)\n",
        "    doc = nlp(frase)\n",
        "\n",
        "    lemma = [word.lemma for sent in doc.sentences for word in sent.words]\n",
        "\n",
        "    return lemma\n"
      ],
      "metadata": {
        "id": "2dm3vMbg8_y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casos de prueba"
      ],
      "metadata": {
        "id": "nfc6LvMnUDAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestLematizacionPasado(unittest.TestCase):\n",
        "\n",
        "    def test_pasado_regular(self):\n",
        "        entrada = [\"trabaj√©\", \"comiendo\", \"caminamos\"]\n",
        "        esperado = [\"trabajar\", \"comer\", \"caminar\"]\n",
        "        resultado = lematizacionProc(entrada)\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_pasado_irregular(self):\n",
        "        entrada = [\"fui\", \"tuve\", \"dije\"]\n",
        "        esperado = [\"ser\", \"tener\", \"decir\"]\n",
        "        resultado = lematizacionProc(entrada)\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_mixto_regulares_irregulares_neutros(self):\n",
        "        entrada = [\"trabajamos\", \"fue\", \"dijeron\", \"casa\", \"feliz\"]\n",
        "        esperado = [\"trabajar\", \"ser\", \"decir\", \"casa\", \"feliz\"]\n",
        "        resultado = lematizacionProc(entrada)\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "\n",
        "unittest.main(argv=[''], exit=False, verbosity=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ULSjgMV8o9n",
        "outputId": "0ba994ba-0e0c-45d9-b8d4-6d3eb1129528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
            "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
            "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
            "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
            "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
            "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
            "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
            "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
            "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
            "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
            "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
            "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
            "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 13 tests in 2.421s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7e0efdeef950>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prueba limpieza total**"
      ],
      "metadata": {
        "id": "WQ3VO_FxUVY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def limpiezaAbsoluta(tweets):\n",
        "  '''\n",
        "    La funci√≥n de limpiezaAbsoluta(tweets), tiene como objetivo hacer el llamdo\n",
        "    de todas las funciones diferentes para la limpieza de los datos provenientes\n",
        "    del archivos csv.\n",
        "    El dato de entrada es una lista de strings y el dato de salida es\n",
        "    una lista de strings.\n",
        "    Por cada proceso de limpieza se crea una variable diferente, con el fin de\n",
        "    evitar confusiones se crean diferentes variables para llevar acabo cada\n",
        "    llamda de las funciones.\n",
        "  '''\n",
        "\n",
        "  limpieza = []\n",
        "  jerga = []\n",
        "  lematizacion = []\n",
        "\n",
        "  #Expresiones regulares\n",
        "  for tweet in tweets:\n",
        "    limpieza.append(limpiar_texto(tweet))\n",
        "\n",
        "  #Jerga\n",
        "  for tweet in limpieza:\n",
        "    jerga.append(jerga_proceso(tweet))\n",
        "\n",
        "  #Lematizaci√≥n\n",
        "\n",
        "  lematizacion\n",
        "\n",
        "  for tweet in jerga:\n",
        "    lematizacion.append(lematizacionProc(tweet))\n",
        "\n",
        "  return lematizacion"
      ],
      "metadata": {
        "id": "M-jI_bR7UU_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casos de prueba"
      ],
      "metadata": {
        "id": "vHquTRQyV3e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestLimpiezaAbsoluta(unittest.TestCase):\n",
        "\n",
        "    def test_tweet1(self):\n",
        "        tweets = [\"Estoy comiendo con @ana üòä\"]\n",
        "        esperado = ['estar', 'comer', 'anorexia']\n",
        "        resultado = limpiezaAbsoluta(tweets)[0]\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_tweet2(self):\n",
        "        tweets = [\"Dorm√≠ feliz con proana y mi cuerpo. üò¥\"]\n",
        "        esperado = ['dormi', 'feliz', 'anorexia', 'cuerpo']\n",
        "        resultado = limpiezaAbsoluta(tweets)[0]\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_tweet3(self):\n",
        "        tweets = [\"Amo la #comida y fui a la casa\"]\n",
        "        esperado = ['amar', 'comida', 'casa']\n",
        "        resultado = limpiezaAbsoluta(tweets)[0]\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_tweet4(self):\n",
        "        tweets = [\"M√≠a dijo que trabajamos bien.\"]\n",
        "        esperado = ['bulimia', 'decir', 'trabajar', 'bien']\n",
        "        resultado = limpiezaAbsoluta(tweets)[0]\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "unittest.main(argv=[''], exit=False, verbosity=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BWl9nfFV1TU",
        "outputId": "c0b9c2db-59d7-401a-8b19-baa9b774e1f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
            "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
            "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
            "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
            "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
            "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
            "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
            "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
            "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
            "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
            "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
            "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
            "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
            "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
            "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
            "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
            "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 17 tests in 5.820s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7e0efdf0d810>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pruebas Bigramas**"
      ],
      "metadata": {
        "id": "0SgInfZ3X4W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def realizarBigramas(resultadoLimpio):\n",
        "  '''\n",
        "  La funci√≥n de realizarBgramas(resultado_limpio) tiene como objetivo encontar\n",
        "  por cada tweet sus bigramas correspondintes\n",
        "  El dato de entrada es una lista de lista de strings y el dato de salida es\n",
        "  una lista de lista de strings, que contiene los bigramas correspondientes a\n",
        "  cada tweet.\n",
        "  '''\n",
        "  arrBigramas = []\n",
        "  for elem in resultadoLimpio:\n",
        "    bigrama = list(ngrams(elem,2))\n",
        "    arrBigramas.append([\" \".join(t) for t in bigrama])\n",
        "\n",
        "  return arrBigramas"
      ],
      "metadata": {
        "id": "n-eZUBcoX7r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casos de prueba"
      ],
      "metadata": {
        "id": "b2Cq_S-EYDRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestRealizarBigramasAnorexia(unittest.TestCase):\n",
        "\n",
        "    def test_bigrama_simple(self):\n",
        "        entrada = [[\"odio\", \"comer\"]]\n",
        "        esperado = [[\"odio comer\"]]\n",
        "        resultado = realizarBigramas(entrada)\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_bigrama_multiple(self):\n",
        "        entrada = [[\"no\", \"quiero\", \"desayunar\"]]\n",
        "        esperado = [[\"no quiero\", \"quiero desayunar\"]]\n",
        "        resultado = realizarBigramas(entrada)\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_palabra_unica(self):\n",
        "        entrada = [[\"delgadez\"]]\n",
        "        esperado = [[]]\n",
        "        resultado = realizarBigramas(entrada)\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_frase_vacia(self):\n",
        "        entrada = [[]]\n",
        "        esperado = [[]]\n",
        "        resultado = realizarBigramas(entrada)\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "unittest.main(argv=[''], exit=False, verbosity=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u-tkU_pYVGT",
        "outputId": "4701f6e1-67a2-4765-9216-d1c0c505fddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
            "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
            "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
            "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
            "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
            "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
            "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
            "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
            "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
            "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
            "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
            "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
            "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
            "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
            "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
            "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
            "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
            "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
            "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
            "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
            "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 21 tests in 5.499s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7e0efdedd9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prueba Semejanza Bigramas**"
      ],
      "metadata": {
        "id": "-dF50PwWauJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def semejanzaBigramas(arrBigramas):\n",
        "  '''\n",
        "  La funci√≥n de semejanzasBigramas(arrBigramas) tiene como objetivo definir los\n",
        "  vectores necesarios para procesar la semejanza entre cada tweet.\n",
        "  El dato de entrada es una lista de lista de strings y el dato de salida es\n",
        "  una lista de vectores.\n",
        "  Se crea una bagOfWords, la cual contiene los bigramas √∫nicos encontrados entre\n",
        "  todos los tweets, despu√©s procede a comparar estos con cada uno de los bigramas\n",
        "  de cada tweet, agrega un 1 si ha sido igual y 0 de lo contrario.\n",
        "  '''\n",
        "  bagOfWords = [];\n",
        "\n",
        "  for elem in arrBigramas:\n",
        "    for bigrama in elem:\n",
        "      if bigrama not in bagOfWords:\n",
        "        bagOfWords.append(bigrama)\n",
        "\n",
        "  vectores = []\n",
        "\n",
        "  for elem in arrBigramas:\n",
        "    vector = []\n",
        "    for bigrama in bagOfWords:\n",
        "      if bigrama in elem:\n",
        "        vector.append(1) #Lo encontr√≥\n",
        "      else:\n",
        "        vector.append(0)\n",
        "    vectores.append(vector)#No\n",
        "\n",
        "  return vectores"
      ],
      "metadata": {
        "id": "ii8aOJhxaxXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casos de prueba"
      ],
      "metadata": {
        "id": "v3iex26gaydQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestSemejanzaBigramas(unittest.TestCase):\n",
        "\n",
        "    def test_bigramas_no_repetidos(self):\n",
        "        entrada = [\n",
        "            [\"ana odia\", \"odia comer\"],\n",
        "            [\"comer manzana\"],\n",
        "            [\"odio mi cuerpo\"]\n",
        "        ]\n",
        "\n",
        "        esperado = [\n",
        "            [1, 1, 0, 0],\n",
        "            [0, 0, 1, 0],\n",
        "            [0, 0, 0, 1]\n",
        "        ]\n",
        "        resultado = semejanzaBigramas(entrada)\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_bigramas_repetidos(self):\n",
        "        entrada = [\n",
        "            [\"me gusta\", \"comer sano\"],\n",
        "            [\"comer sano\", \"me gusta\"],\n",
        "            [\"comer sano\"]\n",
        "        ]\n",
        "\n",
        "        esperado = [\n",
        "            [1, 1],\n",
        "            [1, 1],\n",
        "            [0, 1]\n",
        "        ]\n",
        "        resultado = semejanzaBigramas(entrada)\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_bigramas_unicos(self):\n",
        "        entrada = [\n",
        "            [\"proana es\"],\n",
        "            [\"odio comer\"],\n",
        "            [\"vomit√© todo\"]\n",
        "        ]\n",
        "\n",
        "        esperado = [\n",
        "            [1, 0, 0],\n",
        "            [0, 1, 0],\n",
        "            [0, 0, 1]\n",
        "        ]\n",
        "        resultado = semejanzaBigramas(entrada)\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_vacio(self):\n",
        "        entrada = [[]]\n",
        "        esperado = [[ ]]  # No hay bigramas, no hay columnas\n",
        "        resultado = semejanzaBigramas(entrada)\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "unittest.main(argv=[''], exit=False, verbosity=2)"
      ],
      "metadata": {
        "id": "guwfNwfBa-vx",
        "outputId": "eda21880-f1d3-45ea-f1c1-6a9b3b0f93c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
            "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
            "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
            "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
            "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
            "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
            "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
            "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
            "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
            "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
            "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
            "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
            "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
            "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
            "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
            "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
            "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
            "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
            "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
            "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
            "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
            "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
            "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
            "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
            "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 25 tests in 5.789s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7e0efdf5d290>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prueba Frecuencia de palabras Clave desde una fuente**"
      ],
      "metadata": {
        "id": "bTZFZBItdSQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def palabrasClaveFuente(datosLimpios):\n",
        "  '''\n",
        "  La funci√≥n de palabrasClaveFunte(datosLimpio) tiene como objetivo contabilizar\n",
        "  las palabras clave relacionadas con el trastorno alimenticio de la anorexia.\n",
        "  El dato de entrada es una lista de lista de strings y el dato de salida es un\n",
        "  diccionario con las palabras clave y su conteo.\n",
        "  La definci√≥n de estas palabras proviene de la siguiente fuente:\n",
        "  http://ilitia.cua.uam.mx:8080/jspui/handle/123456789/884\n",
        "  '''\n",
        "  palabrasFrecuentes = {'yo':0, 'nosotros':0, 'social':0, 'amigo':0, 'amigos':0, 'amiga':0, 'humano':0, 'ansiedad':0, 'salud':0, 'sentir':0, 'tristeza':0, 'vida':0, 'sexual':0, 'casa':0, 'hogar':0}\n",
        "\n",
        "  for tweet in datosLimpios:\n",
        "    for palabra in tweet:\n",
        "      if palabra in palabrasFrecuentes:\n",
        "        palabrasFrecuentes[palabra] += 1;\n",
        "\n",
        "  return palabrasFrecuentes\n"
      ],
      "metadata": {
        "id": "qHbnncuzdXaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casos de prueba"
      ],
      "metadata": {
        "id": "Kj-0iMD-daX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestPalabrasClaveFuente(unittest.TestCase):\n",
        "\n",
        "    def test_con_un_tweet(self):\n",
        "        entrada = [[\"yo\", \"siento\", \"tristeza\", \"en\", \"mi\", \"hogar\"]]\n",
        "        resultado = palabrasClaveFuente(entrada)\n",
        "\n",
        "        esperado = {\n",
        "            'yo': 1, 'nosotros': 0, 'social': 0, 'amigo': 0,\n",
        "            'amigos': 0, 'amiga': 0, 'humano': 0, 'ansiedad': 0,\n",
        "            'salud': 0, 'sentir': 0, 'tristeza': 1, 'vida': 0,\n",
        "            'sexual': 0, 'casa': 0, 'hogar': 1\n",
        "        }\n",
        "\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_con_multiples_tweets(self):\n",
        "        entrada = [\n",
        "            [\"yo\", \"amigos\", \"vida\", \"salud\"],\n",
        "            [\"hogar\", \"sexual\", \"yo\", \"sentir\"],\n",
        "            [\"tristeza\", \"ansiedad\", \"amiga\"]\n",
        "        ]\n",
        "        resultado = palabrasClaveFuente(entrada)\n",
        "\n",
        "        esperado = {\n",
        "            'yo': 2, 'nosotros': 0, 'social': 0, 'amigo': 0,\n",
        "            'amigos': 1, 'amiga': 1, 'humano': 0, 'ansiedad': 1,\n",
        "            'salud': 1, 'sentir': 1, 'tristeza': 1, 'vida': 1,\n",
        "            'sexual': 1, 'casa': 0, 'hogar': 1\n",
        "        }\n",
        "\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_sin_palabras_clave(self):\n",
        "        entrada = [[\"pizza\", \"felicidad\", \"caminar\"]]\n",
        "        resultado = palabrasClaveFuente(entrada)\n",
        "\n",
        "        esperado = {\n",
        "            'yo': 0, 'nosotros': 0, 'social': 0, 'amigo': 0,\n",
        "            'amigos': 0, 'amiga': 0, 'humano': 0, 'ansiedad': 0,\n",
        "            'salud': 0, 'sentir': 0, 'tristeza': 0, 'vida': 0,\n",
        "            'sexual': 0, 'casa': 0, 'hogar': 0\n",
        "        }\n",
        "\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_tweet_vacio(self):\n",
        "        entrada = [[]]\n",
        "        resultado = palabrasClaveFuente(entrada)\n",
        "\n",
        "        esperado = {\n",
        "            'yo': 0, 'nosotros': 0, 'social': 0, 'amigo': 0,\n",
        "            'amigos': 0, 'amiga': 0, 'humano': 0, 'ansiedad': 0,\n",
        "            'salud': 0, 'sentir': 0, 'tristeza': 0, 'vida': 0,\n",
        "            'sexual': 0, 'casa': 0, 'hogar': 0\n",
        "        }\n",
        "\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "unittest.main(argv=[''], exit=False, verbosity=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nucetRtdZrm",
        "outputId": "ba944078-7565-444a-b7d6-acc31e70912f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
            "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
            "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
            "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
            "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
            "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
            "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
            "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
            "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
            "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
            "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
            "test_con_multiples_tweets (__main__.TestPalabrasClaveFuente.test_con_multiples_tweets) ... ok\n",
            "test_con_un_tweet (__main__.TestPalabrasClaveFuente.test_con_un_tweet) ... ok\n",
            "test_sin_palabras_clave (__main__.TestPalabrasClaveFuente.test_sin_palabras_clave) ... ok\n",
            "test_tweet_vacio (__main__.TestPalabrasClaveFuente.test_tweet_vacio) ... ok\n",
            "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
            "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
            "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
            "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
            "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
            "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
            "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
            "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
            "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
            "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
            "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
            "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
            "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
            "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 29 tests in 5.645s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7e0efdf3d750>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prueba - Palabras m√°s frecuentes**"
      ],
      "metadata": {
        "id": "5fV4uK-Rf-Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def palabrasClave(datosLimpio):\n",
        "  '''\n",
        "  La funci√≥n de palabrasClave(datosLimpio) tiene como objetivo contabilizar\n",
        "  las palabras clave relacionadas con el trastorno alimenticio de la anorexia,\n",
        "  de acuerdo a la informaci√≥n de los tweets ya preprocesaodos.\n",
        "  El dato de entrada es una lista de lista de strings y el dato de salida es un\n",
        "  diccionario con las palabras clave y su conteo\n",
        "  Se revisa cada tweet y las palabras encontradas con el fin de conocer aquellas\n",
        "  palabras con mayor recurrencia en los tweets.\n",
        "  '''\n",
        "\n",
        "  vocabulary = {}\n",
        "  for tweet in datosLimpio:\n",
        "    for palabra in tweet:\n",
        "      if palabra in vocabulary:\n",
        "        vocabulary[palabra] += 1;\n",
        "      else:\n",
        "        vocabulary[palabra] = 1;\n",
        "\n",
        "  relevantes  = sorted(vocabulary.items(), key=lambda item: item[1], reverse=True)[:10]\n",
        "  return relevantes\n",
        "\n"
      ],
      "metadata": {
        "id": "FzdSfR92gAHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casos de prueba"
      ],
      "metadata": {
        "id": "5qVV1jWphNAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestPalabrasClave(unittest.TestCase):\n",
        "\n",
        "    def test_caso_comun(self):\n",
        "        entrada = [\n",
        "            [\"ana\", \"comer\", \"vida\"],\n",
        "            [\"ana\", \"comer\", \"feliz\"],\n",
        "            [\"comer\", \"vida\", \"ana\"]\n",
        "        ]\n",
        "        esperado = [(\"ana\", 3), (\"comer\", 3), (\"vida\", 2), (\"feliz\", 1)]\n",
        "        resultado = palabrasClave(entrada)\n",
        "        self.assertEqual(resultado[:4], esperado)\n",
        "\n",
        "    def test_empate_en_frecuencia(self):\n",
        "        entrada = [\n",
        "            [\"ana\", \"comer\"],\n",
        "            [\"vida\", \"comer\"],\n",
        "            [\"ana\", \"vida\"]\n",
        "        ]\n",
        "        esperado = [(\"ana\", 2), (\"comer\", 2), (\"vida\", 2)]\n",
        "        resultado = palabrasClave(entrada)\n",
        "        self.assertEqual(resultado[:3], esperado)\n",
        "\n",
        "    def test_menos_de_diez_palabras(self):\n",
        "        entrada = [\n",
        "            [\"una\", \"dos\", \"tres\"],\n",
        "            [\"cuatro\", \"cinco\"]\n",
        "        ]\n",
        "        esperado = [\n",
        "            (\"una\", 1), (\"dos\", 1), (\"tres\", 1),\n",
        "            (\"cuatro\", 1), (\"cinco\", 1)\n",
        "        ]\n",
        "        resultado = palabrasClave(entrada)\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_tweet_vacio(self):\n",
        "        entrada = [[]]\n",
        "        esperado = []\n",
        "        resultado = palabrasClave(entrada)\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "    def test_todo_repetido(self):\n",
        "        entrada = [\n",
        "            [\"ana\", \"ana\", \"ana\"],\n",
        "            [\"ana\", \"ana\"]\n",
        "        ]\n",
        "        esperado = [(\"ana\", 5)]\n",
        "        resultado = palabrasClave(entrada)\n",
        "        self.assertEqual(resultado, esperado)\n",
        "\n",
        "unittest.main(argv=[''], exit=False, verbosity=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El88Vc8_hOiU",
        "outputId": "f44849da-a24f-4738-f05c-de70c7352149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
            "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
            "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
            "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
            "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
            "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
            "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
            "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
            "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
            "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
            "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
            "test_caso_comun (__main__.TestPalabrasClave.test_caso_comun) ... ok\n",
            "test_empate_en_frecuencia (__main__.TestPalabrasClave.test_empate_en_frecuencia) ... ok\n",
            "test_menos_de_diez_palabras (__main__.TestPalabrasClave.test_menos_de_diez_palabras) ... ok\n",
            "test_todo_repetido (__main__.TestPalabrasClave.test_todo_repetido) ... ok\n",
            "test_tweet_vacio (__main__.TestPalabrasClave.test_tweet_vacio) ... ok\n",
            "test_con_multiples_tweets (__main__.TestPalabrasClaveFuente.test_con_multiples_tweets) ... ok\n",
            "test_con_un_tweet (__main__.TestPalabrasClaveFuente.test_con_un_tweet) ... ok\n",
            "test_sin_palabras_clave (__main__.TestPalabrasClaveFuente.test_sin_palabras_clave) ... ok\n",
            "test_tweet_vacio (__main__.TestPalabrasClaveFuente.test_tweet_vacio) ... ok\n",
            "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
            "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
            "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
            "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
            "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
            "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
            "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
            "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
            "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
            "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
            "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
            "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
            "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
            "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 34 tests in 5.224s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7e0efdf76d10>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prueba TF-IDF**"
      ],
      "metadata": {
        "id": "H1emn--_jjna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculoTDIDF(resultadoLimpio):\n",
        "  '''\n",
        "   La funci√≥n de calculoTDIDF(resultadoLimpio) tiene como objetivo identificar\n",
        "   las palabras clave dentro de todo el conjunto de datos y el peso de estas.\n",
        "   El dato de entrada es una lista de lista de strings y el dato de salida es un\n",
        "   dataframe con las palabras clave y su peso correspondiente en cada tweet.\n",
        "   Se busca en tener una oracion con cada tweet y despu√©s se procede a realizar\n",
        "   el calculo de TF-IDF. Con el fin de tener informaci√≥n m√°s legible se crea\n",
        "   un dataframe. Y se regresan unicamente las 10 palabras con mayor peso.\n",
        "  '''\n",
        "  #Obtener una oracion por cada tweet\n",
        "  oraciones = [' '.join(palabras) for palabras in resultadoLimpio]\n",
        "\n",
        "  vectorizer = TfidfVectorizer()\n",
        "\n",
        "  # Ajustar datos\n",
        "  matriz = vectorizer.fit_transform(oraciones)\n",
        "\n",
        "  # Matriz legible\n",
        "  df = pd.DataFrame(matriz.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "  tfidf_sums = df.sum(axis=0)\n",
        "\n",
        "  # Primeras 10 palabras  de TF-IDF\n",
        "  claves = tfidf_sums.sort_values(ascending=False).head(10)\n",
        "\n",
        "  return claves\n",
        "\n"
      ],
      "metadata": {
        "id": "GbVXRj7ajmgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casos de prueba"
      ],
      "metadata": {
        "id": "ZeKRBRLdjqZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestCalculoTFIDF_Simple(unittest.TestCase):\n",
        "\n",
        "    def test_tres_tweets_distintos(self):\n",
        "      entrada = [\n",
        "          [\"ana\", \"odia\", \"comer\"],\n",
        "          [\"comer\", \"manzana\"],\n",
        "          [\"verde\", \"manzana\"]\n",
        "      ]\n",
        "      resultado = calculoTDIDF(entrada)\n",
        "      self.assertTrue(len(resultado) <= 10)\n",
        "      self.assertIn(\"comer\", resultado.index)\n",
        "      self.assertIn(\"manzana\", resultado.index)\n",
        "\n",
        "\n",
        "    def test_repite_palabra_en_un_tweet(self):\n",
        "        entrada = [\n",
        "            [\"comer\", \"comer\", \"comer\"],\n",
        "            [\"sano\"],\n",
        "            [\"vida\"]\n",
        "        ]\n",
        "        resultado = calculoTDIDF(entrada)\n",
        "        palabra_mayor_peso = resultado.index[0]\n",
        "        self.assertEqual(palabra_mayor_peso, \"comer\")\n",
        "\n",
        "    def test_una_palabra_por_tweet(self):\n",
        "        entrada = [\n",
        "            [\"comer\"],\n",
        "            [\"vida\"],\n",
        "            [\"ana\"]\n",
        "        ]\n",
        "        resultado = calculoTDIDF(entrada)\n",
        "        self.assertEqual(set(resultado.index), {\"comer\", \"vida\", \"ana\"})\n",
        "\n",
        "unittest.main(argv=[''], exit=False, verbosity=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu_HVt0MjqMr",
        "outputId": "5c1dcc8e-4605-4065-e7e0-8e68dab4e917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_repite_palabra_en_un_tweet (__main__.TestCalculoTFIDF_Simple.test_repite_palabra_en_un_tweet) ... ok\n",
            "test_tres_tweets_distintos (__main__.TestCalculoTFIDF_Simple.test_tres_tweets_distintos) ... ok\n",
            "test_una_palabra_por_tweet (__main__.TestCalculoTFIDF_Simple.test_una_palabra_por_tweet) ... ok\n",
            "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
            "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
            "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
            "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
            "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
            "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
            "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
            "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
            "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
            "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
            "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
            "test_caso_comun (__main__.TestPalabrasClave.test_caso_comun) ... ok\n",
            "test_empate_en_frecuencia (__main__.TestPalabrasClave.test_empate_en_frecuencia) ... ok\n",
            "test_menos_de_diez_palabras (__main__.TestPalabrasClave.test_menos_de_diez_palabras) ... ok\n",
            "test_todo_repetido (__main__.TestPalabrasClave.test_todo_repetido) ... ok\n",
            "test_tweet_vacio (__main__.TestPalabrasClave.test_tweet_vacio) ... ok\n",
            "test_con_multiples_tweets (__main__.TestPalabrasClaveFuente.test_con_multiples_tweets) ... ok\n",
            "test_con_un_tweet (__main__.TestPalabrasClaveFuente.test_con_un_tweet) ... ok\n",
            "test_sin_palabras_clave (__main__.TestPalabrasClaveFuente.test_sin_palabras_clave) ... ok\n",
            "test_tweet_vacio (__main__.TestPalabrasClaveFuente.test_tweet_vacio) ... ok\n",
            "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
            "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
            "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
            "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
            "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
            "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
            "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
            "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
            "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
            "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
            "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
            "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
            "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
            "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 37 tests in 5.326s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7e0efdf3e3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prueba Vectorizaci√≥n**"
      ],
      "metadata": {
        "id": "i7B4KAcGvN9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizacion(resultadoLimpio):\n",
        "  '''\n",
        "  La funci√≥n de vectorizacion(resultadoLimpio) tiene como objetivo obtener el\n",
        "  valor vectorial por cada una de las palabaras encontradas en los tweets ya\n",
        "  preprocesados.\n",
        "  El dato de entrada es una lista de lista de strings y el dato de salida es un\n",
        "  objeto de tipo Word2Vec.\n",
        "  '''\n",
        "\n",
        "  model = Word2Vec(\n",
        "    sentences = resultadoLimpio,\n",
        "    vector_size = 70,\n",
        "    sg = 1,\n",
        "    window = 10,\n",
        "    min_count = 1,\n",
        "  )\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "xNAthZjIvQy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casos de prueba"
      ],
      "metadata": {
        "id": "XQpfEBg9vn7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestVectorizacion(unittest.TestCase):\n",
        "\n",
        "    def test_es_un_modelo_word2vec(self):\n",
        "        entrada = [[\"ana\", \"odia\", \"comer\"], [\"comer\", \"sano\"]]\n",
        "        modelo = vectorizacion(entrada)\n",
        "        self.assertIsInstance(modelo, Word2Vec)\n",
        "\n",
        "    def test_modelo_funciona_con_tweet_unico(self):\n",
        "        entrada = [[\"comer\", \"sano\", \"vida\", \"feliz\", \"ana\"]]\n",
        "        modelo = vectorizacion(entrada)\n",
        "        for palabra in entrada[0]:\n",
        "            self.assertIn(palabra, modelo.wv.key_to_index)\n",
        "\n",
        "unittest.main(argv=[''], exit=False, verbosity=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q62Xf343vpLW",
        "outputId": "81f8af98-81c4-437b-f5f4-214f269d848e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_repite_palabra_en_un_tweet (__main__.TestCalculoTFIDF_Simple.test_repite_palabra_en_un_tweet) ... ok\n",
            "test_tres_tweets_distintos (__main__.TestCalculoTFIDF_Simple.test_tres_tweets_distintos) ... ok\n",
            "test_una_palabra_por_tweet (__main__.TestCalculoTFIDF_Simple.test_una_palabra_por_tweet) ... ok\n",
            "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
            "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
            "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
            "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
            "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
            "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
            "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
            "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
            "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
            "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
            "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
            "test_caso_comun (__main__.TestPalabrasClave.test_caso_comun) ... ok\n",
            "test_empate_en_frecuencia (__main__.TestPalabrasClave.test_empate_en_frecuencia) ... ok\n",
            "test_menos_de_diez_palabras (__main__.TestPalabrasClave.test_menos_de_diez_palabras) ... ok\n",
            "test_todo_repetido (__main__.TestPalabrasClave.test_todo_repetido) ... ok\n",
            "test_tweet_vacio (__main__.TestPalabrasClave.test_tweet_vacio) ... ok\n",
            "test_con_multiples_tweets (__main__.TestPalabrasClaveFuente.test_con_multiples_tweets) ... ok\n",
            "test_con_un_tweet (__main__.TestPalabrasClaveFuente.test_con_un_tweet) ... ok\n",
            "test_sin_palabras_clave (__main__.TestPalabrasClaveFuente.test_sin_palabras_clave) ... ok\n",
            "test_tweet_vacio (__main__.TestPalabrasClaveFuente.test_tweet_vacio) ... ok\n",
            "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
            "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
            "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
            "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
            "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
            "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
            "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
            "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
            "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
            "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
            "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
            "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
            "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
            "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
            "test_es_un_modelo_word2vec (__main__.TestVectorizacion.test_es_un_modelo_word2vec) ... ok\n",
            "test_modelo_funciona_con_tweet_unico (__main__.TestVectorizacion.test_modelo_funciona_con_tweet_unico) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 39 tests in 5.495s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7e0efdf5cf50>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prueba Transformaci√≥n**"
      ],
      "metadata": {
        "id": "Tyax_SaLyXDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Para las siguientes dos pruebas se debe\n",
        "1. Haber definido previamente la funci√≥n limpiezaAbsoluta(), que limpia los tweets:\n",
        "    - Elimina menciones, hashtags, URLs, puntuaci√≥n, emojis y stopwords.\n",
        "    - Reemplaza jergas como \"ana\" o \"m√≠a\" por \"anorexia\" o \"bulimia\".\n",
        "    - Lematiza las palabras (reduce a su forma base).\n",
        "\n",
        "2. Haber definido vectorizacion(), que entrena un modelo Word2Vec con los tweets limpios."
      ],
      "metadata": {
        "id": "r2ZRMrbD4opu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = [\n",
        "    \"Me siento culpable despu√©s de comer üòî @ana_apoyo #culpa\",\n",
        "    \"La delgadez es mi meta, no quiero engordar jam√°s #thinspo\",\n",
        "    \"Gracias @ana_power por darme fuerza para resistir la comida #proana\",\n",
        "    \"Despu√©s de cada atrac√≥n, m√≠a me libera... @bulimia_team #purga\",\n",
        "    \"Hoy no comer√© nada, solo agua y t√© üçµ #fasting #control\"\n",
        "]\n",
        "\n",
        "resultadoLimpio = limpiezaAbsoluta(tweets)\n",
        "modeloVector = vectorizacion(resultadoLimpio)\n",
        "\n"
      ],
      "metadata": {
        "id": "i3BnFo293KAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformacion(tweets):\n",
        "  '''\n",
        "  La funci√≥n transformacion(tweets) tiene como objetivo que por cada palabra\n",
        "  encontrada en cada tweet, se tenga un valor vectorial.\n",
        "  El dato de entrada es una lista de strings y el dato de salida es un vector\n",
        "  por cada tweet.\n",
        "  Se busca que por cada tweet tener solo un vector que contenga todos los\n",
        "  valores de cada palabra.\n",
        "  '''\n",
        "  total = 0\n",
        "  for palabra in tweets:\n",
        "    total += modeloVector.wv[palabra]\n",
        "  return total"
      ],
      "metadata": {
        "id": "9jssXoewyawt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casos de prueba"
      ],
      "metadata": {
        "id": "QuH_oSN355-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestTransformacionBasica(unittest.TestCase):\n",
        "\n",
        "    def test_vector_devuelto_es_array(self):\n",
        "        tweet = resultadoLimpio[0]\n",
        "        vector = transformacion(tweet)\n",
        "        self.assertIsInstance(vector, np.ndarray)\n",
        "\n",
        "    def test_vector_tiene_datos(self):\n",
        "        tweet = resultadoLimpio[1]\n",
        "        vector = transformacion(tweet)\n",
        "        self.assertTrue(any(vector))\n",
        "\n",
        "    def test_vector_tweet_vacio(self):\n",
        "        vector = transformacion([])\n",
        "        self.assertTrue(np.all(vector == 0))\n",
        "\n",
        "unittest.main(argv=[''], exit=False, verbosity=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkRk3S5r576a",
        "outputId": "6f9ebd59-ff58-4a4c-e4d6-00d3fdddc5e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_repite_palabra_en_un_tweet (__main__.TestCalculoTFIDF_Simple.test_repite_palabra_en_un_tweet) ... ok\n",
            "test_tres_tweets_distintos (__main__.TestCalculoTFIDF_Simple.test_tres_tweets_distintos) ... ok\n",
            "test_una_palabra_por_tweet (__main__.TestCalculoTFIDF_Simple.test_una_palabra_por_tweet) ... ok\n",
            "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
            "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
            "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
            "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
            "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
            "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
            "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
            "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
            "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
            "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
            "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
            "test_caso_comun (__main__.TestPalabrasClave.test_caso_comun) ... ok\n",
            "test_empate_en_frecuencia (__main__.TestPalabrasClave.test_empate_en_frecuencia) ... ok\n",
            "test_menos_de_diez_palabras (__main__.TestPalabrasClave.test_menos_de_diez_palabras) ... ok\n",
            "test_todo_repetido (__main__.TestPalabrasClave.test_todo_repetido) ... ok\n",
            "test_tweet_vacio (__main__.TestPalabrasClave.test_tweet_vacio) ... ok\n",
            "test_con_multiples_tweets (__main__.TestPalabrasClaveFuente.test_con_multiples_tweets) ... ok\n",
            "test_con_un_tweet (__main__.TestPalabrasClaveFuente.test_con_un_tweet) ... ok\n",
            "test_sin_palabras_clave (__main__.TestPalabrasClaveFuente.test_sin_palabras_clave) ... ok\n",
            "test_tweet_vacio (__main__.TestPalabrasClaveFuente.test_tweet_vacio) ... ok\n",
            "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
            "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
            "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
            "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
            "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
            "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
            "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
            "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
            "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
            "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
            "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
            "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
            "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
            "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
            "test_vector_devuelto_es_array (__main__.TestTransformacionBasica.test_vector_devuelto_es_array) ... ok\n",
            "test_vector_tiene_datos (__main__.TestTransformacionBasica.test_vector_tiene_datos) ... ok\n",
            "test_vector_tweet_vacio (__main__.TestTransformacionBasica.test_vector_tweet_vacio) ... ok\n",
            "test_es_un_modelo_word2vec (__main__.TestVectorizacion.test_es_un_modelo_word2vec) ... ok\n",
            "test_modelo_funciona_con_tweet_unico (__main__.TestVectorizacion.test_modelo_funciona_con_tweet_unico) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 42 tests in 6.709s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7e0efdf50210>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prueba Procesamiento**"
      ],
      "metadata": {
        "id": "2iqQ_VE4-Aen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def procesamiento(resultadoLimpio):\n",
        "  '''\n",
        "  La funci√≥n procesamiento(resultadoLimpio) tiene como objetivo descomponer la\n",
        "  lista tweets.\n",
        "  El dato de entrada es una lista de lista de strings y el dato de salida es una\n",
        "  lista de vectores\n",
        "  Se busca que por cada tweet se obtenga su respectivo valor vectorial, respecto\n",
        "  a las palabras que contiene.\n",
        "  '''\n",
        "  numeroTweets = []\n",
        "  for tweet in resultadoLimpio:\n",
        "    numeroTweets.append(transformacion(tweet))\n",
        "  return numeroTweets\n"
      ],
      "metadata": {
        "id": "n6B6XGIN-DQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casos de prueba"
      ],
      "metadata": {
        "id": "5XTwuAba_Lwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestProcesamiento(unittest.TestCase):\n",
        "\n",
        "    def test_lista_de_vectores(self):\n",
        "        vectores = procesamiento(resultadoLimpio)\n",
        "        self.assertEqual(len(vectores), len(resultadoLimpio))\n",
        "\n",
        "    def test_vector_es_ndarray(self):\n",
        "        vectores = procesamiento(resultadoLimpio)\n",
        "        self.assertIsInstance(vectores[0], np.ndarray)\n",
        "\n",
        "    def test_vector_tiene_datos(self):\n",
        "        vectores = procesamiento(resultadoLimpio)\n",
        "        self.assertTrue(any(vectores[0]))\n",
        "\n",
        "unittest.main(argv=[''], exit=False, verbosity=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjDZY-nc_LDm",
        "outputId": "e217735c-c11a-4509-8630-93de54aa08f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_repite_palabra_en_un_tweet (__main__.TestCalculoTFIDF_Simple.test_repite_palabra_en_un_tweet) ... ok\n",
            "test_tres_tweets_distintos (__main__.TestCalculoTFIDF_Simple.test_tres_tweets_distintos) ... ok\n",
            "test_una_palabra_por_tweet (__main__.TestCalculoTFIDF_Simple.test_una_palabra_por_tweet) ... ok\n",
            "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
            "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
            "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
            "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
            "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
            "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
            "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
            "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
            "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
            "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
            "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
            "test_caso_comun (__main__.TestPalabrasClave.test_caso_comun) ... ok\n",
            "test_empate_en_frecuencia (__main__.TestPalabrasClave.test_empate_en_frecuencia) ... ok\n",
            "test_menos_de_diez_palabras (__main__.TestPalabrasClave.test_menos_de_diez_palabras) ... ok\n",
            "test_todo_repetido (__main__.TestPalabrasClave.test_todo_repetido) ... ok\n",
            "test_tweet_vacio (__main__.TestPalabrasClave.test_tweet_vacio) ... ok\n",
            "test_con_multiples_tweets (__main__.TestPalabrasClaveFuente.test_con_multiples_tweets) ... ok\n",
            "test_con_un_tweet (__main__.TestPalabrasClaveFuente.test_con_un_tweet) ... ok\n",
            "test_sin_palabras_clave (__main__.TestPalabrasClaveFuente.test_sin_palabras_clave) ... ok\n",
            "test_tweet_vacio (__main__.TestPalabrasClaveFuente.test_tweet_vacio) ... ok\n",
            "test_lista_de_vectores (__main__.TestProcesamiento.test_lista_de_vectores) ... ok\n",
            "test_vector_es_ndarray (__main__.TestProcesamiento.test_vector_es_ndarray) ... ok\n",
            "test_vector_tiene_datos (__main__.TestProcesamiento.test_vector_tiene_datos) ... ok\n",
            "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
            "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
            "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
            "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
            "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
            "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
            "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
            "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
            "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
            "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
            "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
            "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
            "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
            "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
            "test_vector_devuelto_es_array (__main__.TestTransformacionBasica.test_vector_devuelto_es_array) ... ok\n",
            "test_vector_tiene_datos (__main__.TestTransformacionBasica.test_vector_tiene_datos) ... ok\n",
            "test_vector_tweet_vacio (__main__.TestTransformacionBasica.test_vector_tweet_vacio) ... ok\n",
            "test_es_un_modelo_word2vec (__main__.TestVectorizacion.test_es_un_modelo_word2vec) ... ok\n",
            "test_modelo_funciona_con_tweet_unico (__main__.TestVectorizacion.test_modelo_funciona_con_tweet_unico) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 45 tests in 5.281s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7e0efdf50310>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}