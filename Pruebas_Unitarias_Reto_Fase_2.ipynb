{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ap2fwoJK5jE"
   },
   "source": [
    "#Pruebas Unitarias - Fase 2\n",
    "Equipo 6:<br>\n",
    "Ingrid García Hernández, A01754475<br>\n",
    "Abigail Donají Chavez Rubio, A01747423<br>\n",
    "Noh Ah Kim Kwon, A01747512<br>\n",
    "Eduardo Alfredo Ramírez Muñoz, A01754917\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sE6otXkDb4ZQ"
   },
   "source": [
    "# **Definición de librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aoB32rcGSOOK",
    "outputId": "cfa225b7-73da-49ce-99d2-d6df4be44d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZtBQu87SREu",
    "outputId": "426c79d8-e06a-4c8f-ae83-1ee277aeb0b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from stanza) (2.0.2)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (5.29.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from stanza) (3.4.2)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (4.13.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
      "Downloading stanza-1.10.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, emoji, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stanza\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed emoji-2.14.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stanza-1.10.1\n"
     ]
    }
   ],
   "source": [
    "pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 878
    },
    "id": "mTOl1fMZS417",
    "outputId": "127a7821-66c6-4a63-da15-c1da3795beeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.3\n",
      "    Uninstalling scipy-1.15.3:\n",
      "      Successfully uninstalled scipy-1.15.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n",
      "Collecting numpy==1.25.2\n",
      "  Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\n",
      "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.2 which is incompatible.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.25.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "697041063daa48abbe0e2b7b43315b9c",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install --upgrade numpy==1.25.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 939,
     "referenced_widgets": [
      "f7ca8de1d76143dc8b6017e4820aec3c",
      "bbb70c3ef5fa4160a5988d80c2fcec1d",
      "ce1e41ec83184465a368234f18817a6e",
      "a863056da8bf415ba6433ca80250a06b",
      "9cec354912dc4eecac543472e4e15cd8",
      "edcd3521c45649e98c41154018c41b78",
      "1dbe09e391c146d8aa8e09b208524a51",
      "8e5dd8a9592d4de8b07939f18ca39c9c",
      "f8b68a6482ce4461b441d3cc1691a295",
      "fe733c2adbea4a40b256f3ba103610e5",
      "e20c8d714b1c41ddb628e7f32b06fda6",
      "65cb7a69ea4a4e5fab914e254cbe61e7",
      "3f75da9ae3734ac48207f901896378e2",
      "d6d7322511ee44f0a873d919833a8c2f",
      "9ff520a9bfeb4451a9b9ed638f52836e",
      "1bdb79e0dea145df8e7dae9ebd8665e5",
      "afbff069faa04da281f4eef0ba42b86a",
      "c57e2bfbd321410b91f3f7d60c20211f",
      "2c181f444bea4447b3a6d417cc8b8f76",
      "fb03382865f444d1932362e421720cfe",
      "ce821ea808ac49eda6ddbc36b0692b60",
      "2c10449b5db84714b157b595b9420393",
      "bee2a70ea67c4118ba48350cb522815f",
      "e799bba13a3a4166abd6068889712343",
      "8532843585c34e4ab751e64d35554b11",
      "0846a349cce24678a1e60306e36370da",
      "0f005a175b244fe59fa56d8bb176d7a3",
      "ff0f86b8313e4f9d8c35e356769fd673",
      "68b41e68fb7e42a58cf00bebfd0ea511",
      "641819c900cf4708baa1c8a8323bf312",
      "97c5ed9f48c74c9d95159105f5c3e64e",
      "13dddba1cece4e79b3281afae96d4102",
      "7f504ffde27449a1aef5d0fea0754b5b",
      "a58d853d6b1a448886e35390101197bc",
      "92357613336d41e3b999c77dde472c4d",
      "66a6e0818f7d4f7ca9d35e323f182ba4",
      "ba42df56e9504418ac174d869fee084d",
      "7b5f82f39d48493981158d51ca47d7c2",
      "ac9c56bd36ae4d038000cc5c784c913b",
      "d9d0b30de63142dea535aa153b2fadd6",
      "141f28e558cf41e3b2165c3fb3d65b80",
      "f3a9522051dc4ebbaa803a4333fa4436",
      "a5e927d7cf5e435ebe4ffd737b1b0b2b",
      "9ab118442bc04170aed0952916bef4f9",
      "ed2dbb5e12e243cfac7d00cb75277ed9",
      "c38d619a501a40e5bcbb650b9a763904",
      "0071c9ed506a40daac1e518cbe28dbb6",
      "ebfe8f88fcf74cb6ab244b2e3637a0f2",
      "06426c1548d849f5acfa7f5c365b481d",
      "e1ae64323a16463d9b7b4a96a5b519da",
      "599d0ac9ddcf431ab0d346ad7ebb0377",
      "6c31e63217da4653a21180d6d053cea3",
      "2f16cf03d8bf473a9fbb677291d302dc",
      "d044e27464fe472682f4893277ec478b",
      "75eab697e84e43a384351a053767877d",
      "65696222e9de4b44b4db7876e08fd833",
      "4e158204c0e94b1a862748704af6f77b",
      "b8872d120f5246eabde22b3174b49708",
      "3069b89682324e409c773c5e48adf40a",
      "852727061cab444fa997b0f25d3d0dbd",
      "8c2ad6f043394aaab511f4080cafc879",
      "959d9875c3734c0b930772d8e7c7ac7a",
      "893cffac9c2148d9b502e5405f791d67",
      "c9d90654b0464ac48456e272a9fa41b3",
      "949d7d56028f49dc81a09a8065aadac4",
      "e2fb8d70f90d4442b60256bf9cff114a",
      "fd453e5616494af88e2b19ad15355f2f",
      "21ceb243133e4b97885e8ebb045fee62",
      "40812b6d09a54c2e9c872d52098754d9",
      "37b4a08859194bdf91241da535a59212",
      "ee9d7652ff304618b27045271be1ebe1",
      "9274fec0e39b4dc5b6dcb2143f57a77e",
      "830963bac0b74651b6a944a36264f0b4",
      "c742674955564c58a1b51745db85dbd5",
      "703c7805795e411fa2e02e3a7da556cc",
      "53b743ee99b24a31be2c4947d67fddac",
      "6c366c62f36e4fb486d91ed521907ff3",
      "8dfe2d5c38084bca889471d34396a144",
      "7149a592cf2f42e98c0246b600820c65",
      "05a2ce1c53a64568ba5159739afcacbf",
      "fe5c6e1750d84c429e446896bc83d697",
      "138d3af0018a42c3926b959491cf0294",
      "12646301724a469a93c440985e74bf0f",
      "4b95af8e4db646339e2771027f929e16",
      "388705a8fec149bf8de95d51640e58cf",
      "945498f590414f24b578dad03b465f59",
      "31ea5c13f55948f696bf1f4f6448e049",
      "9922f66da0bf41d8aca394424c63c784",
      "61d9d6aad7e94171ad5180c1c9b4fe5e",
      "699a869afc2141ddb7c5b7d4a1ef8444",
      "8e4f5cbc2a7547449883125c50a8a0d9",
      "fe2425faef1940c2b69104a46deccace",
      "bba5ab80832744f4bcdc8950d86aef47",
      "732fad212da546828ade07e716a0ac06",
      "3a949be57f6a47a68833e86cf06fecc0",
      "0e1f784dc317479494e4734c11131c8b",
      "efbc82ca3ae74c6d82074da57182e981",
      "14ab55e443ef45f7aa54e0cdcdc262d8",
      "dbfc98b34f194eaab372835681b37558",
      "c23d0fa39f5544b7b654ebf9316f347e",
      "1787d1e244a94034a700b853b72d4dfb",
      "d94b46993d2c4b0cb33c1d2ef42f2dc7",
      "3be15c0393e84d778a7a913d9634b6e2",
      "b9dd33c608004d299d5c76b7067110cc",
      "bc2f9637abf744efad492f3ce1633dec",
      "a9a262e4b473452daeaecac6ed7f5c8f",
      "073c3218f06848a0a5c1e2ca4541d05e",
      "6ea578f40fb94b568d45ce5bd264500f",
      "3700d2365a474ff2ac0f8e4d423d6dd4",
      "cf76e8456c77469e9e7ecf87e01fc22c",
      "2ceed8b29e84476e9630d72fd30ceca0",
      "7a66278eff9f47e99d25826b8db78b6d",
      "35095692ea6243cbbc676eb78b63439f",
      "75d759db43684af59ff95654e9405222",
      "3decd0aefe0e4c639120c49517bac5fe",
      "d183b6239a464f96b98d0ba4687d81d1",
      "eddad681de5d4fbaa586703e0e579e34",
      "eec7402478954ca482f308e1837364d3",
      "f29271748bd24f5796875952d3b68173",
      "ee0ba733d1e7466b869e0e34972a65fb",
      "54c870d922d648c78b627afe6a39ddec",
      "f7424ca2160d4fb99db0c60346e36829",
      "9b2f3b693e55422986e6e6e9b4f28f3d",
      "9f3101c574b747b996ffb4a369702990",
      "10b24de4dc694de6b492b66d4e0dbee3",
      "bc01551f248549778fd75046b0f6ce44",
      "e64984eddea54361b014715cbf34ba8f",
      "53ecfee847cf4177a6045f882fbf73e7",
      "71f2240c841d48afb3c7dd41ba7dd24f",
      "8d9dcae7b2a14a71a992e45d9aa0b1ff",
      "c360480baa1b4a3c8597fbdd38f21f8b",
      "46da20f01bd649eb838bd996a861059c",
      "c6ef105cd986437c92f8016f44ad1983",
      "fb3ffd47f0314e60bcbf1fec3011150b",
      "2a3e6b1343b643e1b79ac86a01928e45",
      "828c57b0553c45378641151ae89dd63c",
      "2284a6171ba14c9d8e831402c45ee225",
      "95ae356687fe4ebcb98ddd3350415326",
      "d660c2193c334e3e91629f1cf01d2bde",
      "d8b0a7ee603a48f3b1c703e9e0260b7d",
      "5101031022d1456389a86134177ba47e",
      "715f7372d960491297c7e5d133391f37",
      "84ba6ba806204a95a51ae6abdd08cfb0"
     ]
    },
    "id": "oz-OqGYQc9l6",
    "outputId": "b27ffd07-2fac-49a3-8804-53563906e5dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ca8de1d76143dc8b6017e4820aec3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cb7a69ea4a4e5fab914e254cbe61e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/tokenize/combined.pt:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee2a70ea67c4118ba48350cb522815f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/mwt/combined.pt:   0%|        …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58d853d6b1a448886e35390101197bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/pos/combined_charlm.pt:   0%| …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2dbb5e12e243cfac7d00cb75277ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/lemma/combined_nocharlm.pt:   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65696222e9de4b44b4db7876e08fd833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/constituency/combined_charlm.p…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd453e5616494af88e2b19ad15355f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/depparse/combined_charlm.pt:  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfe2d5c38084bca889471d34396a144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/sentiment/tass2020_charlm.pt: …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d9d6aad7e94171ad5180c1c9b4fe5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/ner/conll02.pt:   0%|         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23d0fa39f5544b7b654ebf9316f347e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/backward_charlm/newswiki.pt:  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ceed8b29e84476e9630d72fd30ceca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/pretrain/fasttextwiki.pt:   0%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7424ca2160d4fb99db0c60346e36829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/pretrain/conll17.pt:   0%|    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ef105cd986437c92f8016f44ad1983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-es/resolve/v1.10.0/models/forward_charlm/newswiki.pt:   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "INFO:stanza:Using device: cpu\n",
      "INFO:stanza:Loading: tokenize\n",
      "INFO:stanza:Loading: mwt\n",
      "INFO:stanza:Loading: pos\n",
      "INFO:stanza:Loading: lemma\n",
      "INFO:stanza:Loading: constituency\n",
      "INFO:stanza:Loading: depparse\n",
      "INFO:stanza:Loading: sentiment\n",
      "INFO:stanza:Loading: ner\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "\n",
    "from six import StringIO\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "import pydotplus\n",
    "\n",
    "\n",
    "import stanza\n",
    "nlp = stanza.Pipeline('es')\n",
    "\n",
    "import string\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "\n",
    "import unittest\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U421al6XcdOI"
   },
   "source": [
    "# **Prueba limpieza**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8Gm7nofcgyB"
   },
   "outputs": [],
   "source": [
    "def limpiar_texto(texto):\n",
    "    '''\n",
    "    La función de limpiar_texto(texto), tiene como objetivo eliminar el ruido\n",
    "    dentro de cada elemento de la lista.\n",
    "    El dato de entrada es una lista de strings y el dato de salida es\n",
    "    una lista de strings.\n",
    "    Por cada palabra se debe de normalizar, es decir, se hace caso omiso de los\n",
    "    acentos y después se revisa cada palabra con respecto a  las reglas\n",
    "    definidas. Esto se hace por medio de expresiones regulares. Igualmente, se\n",
    "    eliminan las palabras 'innecesarias',dado que no proporcionan mayor\n",
    "    información o contexto.\n",
    "    '''\n",
    "    #texto.lower()\n",
    "    tweet = []\n",
    "    for palabra in texto.split():\n",
    "      nuevo = palabra\n",
    "      #Acentos\n",
    "      nuevo = unicodedata.normalize ('NFKD', nuevo).encode ('ascii', 'ignore').decode ('utf-8', 'ignore')\n",
    "      for i, j in reglas.items():\n",
    "        if i == 'espacios_extra':\n",
    "          nuevo = re.sub(j, ' ', nuevo)\n",
    "        else:\n",
    "          nuevo = re.sub(j, '',nuevo)\n",
    "      if nuevo not in stopwords.words('spanish'):\n",
    "        tweet.append(nuevo.lower())\n",
    "\n",
    "    return tweet\n",
    "\n",
    "reglas = {\n",
    "    'hashtags': r'#',\n",
    "    'menciones': r'@',\n",
    "    'urls': r'http\\S+|www.\\S+',\n",
    "    'emojis': r'[^\\x00-\\x7F]+',\n",
    "    'puntuacion': r'[^\\w\\s]',\n",
    "    'espacios_extras': r'\\s+' #Tabs, dobles espacios\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0WNyKFAd6kx"
   },
   "source": [
    "Casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XsdANEVKcweJ",
    "outputId": "6a287be9-9b5e-460d-e401-6950bcd462d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
      "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
      "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
      "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
      "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
      "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.024s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x784685044c10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestReglasLimpieza(unittest.TestCase):\n",
    "\n",
    "    def test_hashtags(self):\n",
    "    #Que los hashtags (#) sean eliminados correctamente\n",
    "        texto = \"Amo la #comida y mi #cuerpo\"\n",
    "        salida = limpiar_texto(texto)\n",
    "        esperado = [\"amo\", \"comida\", \"cuerpo\"]\n",
    "        self.assertEqual(salida, esperado)\n",
    "\n",
    "    def test_menciones(self):\n",
    "    #Que las menciones (@usuario) sean removidas del texto\n",
    "        texto = \"Gracias @eduardo por tu apoyo\"\n",
    "        salida = limpiar_texto(texto)\n",
    "        esperado = [\"gracias\", \"eduardo\" , \"apoyo\"]\n",
    "        self.assertEqual(salida, esperado)\n",
    "\n",
    "    def test_urls(self):\n",
    "    #Que se eliminen correctamente URLs con http o www\n",
    "        texto = \"Visita http://teApoyamos.com o www.nutricion.com\"\n",
    "        salida = limpiar_texto(texto)\n",
    "        esperado = ['visita', '', '']\n",
    "        self.assertEqual(salida, esperado)\n",
    "\n",
    "    def test_emojis(self):\n",
    "    #Que se eliminen los emojis (caracteres no ASCII)\n",
    "        texto = \"Estoy muy feliz 😄 por este logro 👏\"\n",
    "        salida = limpiar_texto(texto)\n",
    "        esperado = ['estoy', 'feliz', '', 'logro', '']\n",
    "        self.assertEqual(salida, esperado)\n",
    "\n",
    "    def test_puntuacion(self):\n",
    "    #Que se eliminen signos de puntuación como !, ?, .\n",
    "        texto = \"¡Hola! ¿Cómo estás? Bien, gracias.\"\n",
    "        salida = limpiar_texto(texto)\n",
    "        esperado = [\"hola\", \"como\", \"bien\", \"gracias\"]\n",
    "        self.assertEqual(salida, esperado)\n",
    "\n",
    "    def test_espacios_extras(self):\n",
    "    #Que los espacios extra sean reducidos a un solo espacio\n",
    "        texto = \"Hola     mundo, esto    es   una     prueba.\"\n",
    "        salida = limpiar_texto(texto)\n",
    "        esperado = [\"hola\", \"mundo\", \"prueba\"]\n",
    "        self.assertEqual(salida, esperado)\n",
    "\n",
    "unittest.main(argv=[''], exit=False, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SHHkO19xC-A"
   },
   "source": [
    "# **Prueba jergas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DA4jzaAkxG5Y"
   },
   "outputs": [],
   "source": [
    "def jerga_proceso(texto):\n",
    "  '''\n",
    "    La función de jerga(texto), tiene como objetivo proporcionar información más\n",
    "    precisa de palabras relaciondas con este desorden alimenticio, mas no lo\n",
    "    hacen de manera directa.\n",
    "    El dato de entrada es una lista de lista de strings y el dato de salida es\n",
    "    una lista de strings.\n",
    "    Se asuma que la entrada es una lista de palabras y por ende revisa cada una\n",
    "    de estas para revisar que no se haga caso omiso de ningún caso de jerga.\n",
    "    '''\n",
    "  tweet = []\n",
    "  for palabra in texto:\n",
    "    if palabra == 'ana':\n",
    "      palabra = 'anorexia'\n",
    "    if palabra == 'anorexic':\n",
    "      palabra = 'anorexia'\n",
    "    if palabra == 'mía':\n",
    "      palabra = 'bulimia'\n",
    "    if palabra == 'bulimic':\n",
    "      palabra = 'bulimia'\n",
    "    if palabra == 'mia':\n",
    "      palabra = 'bulimia'\n",
    "    if palabra == 'promia':\n",
    "      palabra = 'bulimia'\n",
    "    if palabra == 'bulymia':\n",
    "      palabra = 'bulimia'\n",
    "    if palabra == 'proana':\n",
    "      palabra = 'anorexia'\n",
    "    if palabra == 'styleana':\n",
    "      palabra = 'anorexia'\n",
    "    if palabra == 'miaana':\n",
    "      palabra = 'anorexia'\n",
    "    if palabra == 'anoreccia':\n",
    "      palabra = 'anorexia'\n",
    "    tweet.append(palabra)\n",
    "\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hI0UWZzs7xTH"
   },
   "source": [
    "Casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhX5UrCUz12m",
    "outputId": "5eda3b27-a5be-4fd7-fef6-1a65f4c556c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
      "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
      "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
      "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
      "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
      "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
      "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
      "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
      "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
      "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 10 tests in 0.028s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x78469c52c850>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestJergaProceso(unittest.TestCase):\n",
    "\n",
    "    def test_palabras_convertidas_a_anorexia(self):\n",
    "    #Que todos los términos asociados a \"anorexia\" se conviertan correctamente\n",
    "        entrada = [\"ana\", \"anorexic\", \"proana\", \"styleana\", \"miaana\", \"anoreccia\"]\n",
    "        salida = jerga_proceso(entrada)\n",
    "        for palabra in salida:\n",
    "            self.assertEqual(palabra, \"anorexia\")\n",
    "\n",
    "    def test_palabras_convertidas_a_bulimia(self):\n",
    "    #Que los términos relacionados con \"bulimia\" se traduzcan correctamente\n",
    "        entrada = [\"mia\", \"mía\", \"bulimic\", \"promia\", \"bulymia\"]\n",
    "        salida = jerga_proceso(entrada)\n",
    "        for palabra in salida:\n",
    "            self.assertEqual(palabra, \"bulimia\")\n",
    "\n",
    "    def test_palabras_no_convertidas(self):\n",
    "    #Que palabras no relacionadas con la jerga no se modifiquen\n",
    "        entrada = [\"feliz\", \"comer\", \"vida\"]\n",
    "        salida = jerga_proceso(entrada)\n",
    "        self.assertEqual(salida, entrada)\n",
    "\n",
    "    def test_mixto_convertidos_y_normales(self):\n",
    "    #Que se combinen correctamente palabras convertidas y no convertidas\n",
    "        entrada = [\"ana\", \"feliz\", \"mia\", \"vida\"]\n",
    "        salida = jerga_proceso(entrada)\n",
    "        self.assertEqual(salida, [\"anorexia\", \"feliz\", \"bulimia\", \"vida\"])\n",
    "\n",
    "unittest.main(argv=[''], exit=False, verbosity=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0r54xTN7-z2"
   },
   "source": [
    "# **Prueba lematización**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dm3vMbg8_y-"
   },
   "outputs": [],
   "source": [
    "def lematizacionProc(tweet):\n",
    "    '''\n",
    "    La función lematizacionProc(tweet), tiene como objetivo lematizar cada tweet,\n",
    "    esto con el fin de obtener la forma de cada palabra.\n",
    "    El dato de entrada es una lista de strings y el dato de salida es\n",
    "    una lista de strings.\n",
    "    Dado que es un proceso de lematización se requiere contar con la oración\n",
    "    completa, para después proceder a unir cada lemma encontrado en una lista.\n",
    "    '''\n",
    "\n",
    "    frase = ' '.join(tweet)\n",
    "    doc = nlp(frase)\n",
    "\n",
    "    lemma = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "    return lemma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfc6LvMnUDAf"
   },
   "source": [
    "Casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ULSjgMV8o9n",
    "outputId": "96f7d2bc-c67e-4f2e-d5db-9f9350840c76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
      "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
      "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
      "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
      "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
      "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
      "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
      "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
      "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
      "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
      "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
      "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
      "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 13 tests in 2.191s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x784685040790>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestLematizacionPasado(unittest.TestCase):\n",
    "\n",
    "    def test_pasado_regular(self):\n",
    "    #Que verbos en pasado regular como \"trabajé\" o \"caminamos\" se transformen correctamente a su forma base\n",
    "        entrada = [\"trabajé\", \"comiendo\", \"caminamos\"]\n",
    "        esperado = [\"trabajar\", \"comer\", \"caminar\"]\n",
    "        resultado = lematizacionProc(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_pasado_irregular(self):\n",
    "    #Que verbos irregulares como \"fui\", \"tuve\", \"dije\" se transformen correctamente en \"ser\", \"tener\", \"decir\"\n",
    "        entrada = [\"fui\", \"tuve\", \"dije\"]\n",
    "        esperado = [\"ser\", \"tener\", \"decir\"]\n",
    "        resultado = lematizacionProc(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_mixto_regulares_irregulares_neutros(self):\n",
    "    #Que en una lista con todo tipo de verbos se transformen o no según sea el caso.\n",
    "        entrada = [\"trabajamos\", \"fue\", \"dijeron\", \"casa\", \"feliz\"]\n",
    "        esperado = [\"trabajar\", \"ser\", \"decir\", \"casa\", \"feliz\"]\n",
    "        resultado = lematizacionProc(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], exit=False, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQ3VO_FxUVY2"
   },
   "source": [
    "# **Prueba limpieza total**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-jI_bR7UU_O"
   },
   "outputs": [],
   "source": [
    "def limpiezaAbsoluta(tweets):\n",
    "  '''\n",
    "    La función de limpiezaAbsoluta(tweets), tiene como objetivo hacer el llamdo\n",
    "    de todas las funciones diferentes para la limpieza de los datos provenientes\n",
    "    del archivos csv.\n",
    "    El dato de entrada es una lista de strings y el dato de salida es\n",
    "    una lista de strings.\n",
    "    Por cada proceso de limpieza se crea una variable diferente, con el fin de\n",
    "    evitar confusiones se crean diferentes variables para llevar acabo cada\n",
    "    llamda de las funciones.\n",
    "  '''\n",
    "\n",
    "  limpieza = []\n",
    "  jerga = []\n",
    "  lematizacion = []\n",
    "\n",
    "  #Expresiones regulares\n",
    "  for tweet in tweets:\n",
    "    limpieza.append(limpiar_texto(tweet))\n",
    "\n",
    "  #Jerga\n",
    "  for tweet in limpieza:\n",
    "    jerga.append(jerga_proceso(tweet))\n",
    "\n",
    "  #Lematización\n",
    "\n",
    "  lematizacion\n",
    "\n",
    "  for tweet in jerga:\n",
    "    lematizacion.append(lematizacionProc(tweet))\n",
    "\n",
    "  return lematizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHquTRQyV3e1"
   },
   "source": [
    "Casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BWl9nfFV1TU",
    "outputId": "903e4671-1e84-408f-906d-0fb96b04b3d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
      "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
      "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
      "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
      "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
      "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
      "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
      "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
      "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
      "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
      "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
      "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
      "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
      "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
      "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
      "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
      "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 17 tests in 4.940s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x784685045310>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestLimpiezaAbsoluta(unittest.TestCase):\n",
    "\n",
    "    def test_tweet1(self):\n",
    "    #Que se realice la limpieza de manera correcta\n",
    "        tweets = [\"Estoy comiendo con @ana 😊\"]\n",
    "        esperado = ['estar', 'comer', 'anorexia']\n",
    "        resultado = limpiezaAbsoluta(tweets)[0]\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_tweet2(self):\n",
    "        tweets = [\"Dormí feliz con proana y mi cuerpo. 😴\"]\n",
    "        esperado = ['dormi', 'feliz', 'anorexia', 'cuerpo']\n",
    "        resultado = limpiezaAbsoluta(tweets)[0]\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_tweet3(self):\n",
    "        tweets = [\"Amo la #comida y fui a la casa\"]\n",
    "        esperado = ['amar', 'comida', 'casa']\n",
    "        resultado = limpiezaAbsoluta(tweets)[0]\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_tweet4(self):\n",
    "        tweets = [\"Mía dijo que trabajamos bien.\"]\n",
    "        esperado = ['bulimia', 'decir', 'trabajar', 'bien']\n",
    "        resultado = limpiezaAbsoluta(tweets)[0]\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "unittest.main(argv=[''], exit=False, verbosity=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SgInfZ3X4W7"
   },
   "source": [
    "# **Pruebas Bigramas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-eZUBcoX7r7"
   },
   "outputs": [],
   "source": [
    "def realizarBigramas(resultadoLimpio):\n",
    "  '''\n",
    "  La función de realizarBgramas(resultado_limpio) tiene como objetivo encontar\n",
    "  por cada tweet sus bigramas correspondintes\n",
    "  El dato de entrada es una lista de lista de strings y el dato de salida es\n",
    "  una lista de lista de strings, que contiene los bigramas correspondientes a\n",
    "  cada tweet.\n",
    "  '''\n",
    "  arrBigramas = []\n",
    "  for elem in resultadoLimpio:\n",
    "    bigrama = list(ngrams(elem,2))\n",
    "    arrBigramas.append([\" \".join(t) for t in bigrama])\n",
    "\n",
    "  return arrBigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2Cq_S-EYDRj"
   },
   "source": [
    "Casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4u-tkU_pYVGT",
    "outputId": "1b80b001-ff78-4923-d1d1-4e478929924a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
      "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
      "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
      "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
      "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
      "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
      "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
      "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
      "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
      "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
      "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
      "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
      "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
      "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
      "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
      "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
      "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
      "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
      "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
      "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
      "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 21 tests in 4.431s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7847a10654d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestRealizarBigramasAnorexia(unittest.TestCase):\n",
    "\n",
    "    def test_bigrama_simple(self):\n",
    "    #Que se genere correctamente un único bigrama a partir de dos palabras\n",
    "        entrada = [[\"odio\", \"comer\"]]\n",
    "        esperado = [[\"odio comer\"]]\n",
    "        resultado = realizarBigramas(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_bigrama_multiple(self):\n",
    "    #Que se generen múltiples bigramas\n",
    "        entrada = [[\"no\", \"quiero\", \"desayunar\"]]\n",
    "        esperado = [[\"no quiero\", \"quiero desayunar\"]]\n",
    "        resultado = realizarBigramas(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_palabra_unica(self):\n",
    "    #Que no se generen bigramas si el tweet contiene solo una palabra\n",
    "        entrada = [[\"delgadez\"]]\n",
    "        esperado = [[]]\n",
    "        resultado = realizarBigramas(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_frase_vacia(self):\n",
    "    #Que la función no falle con entradas vacías y devuelva lista vacía\n",
    "        entrada = [[]]\n",
    "        esperado = [[]]\n",
    "        resultado = realizarBigramas(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "unittest.main(argv=[''], exit=False, verbosity=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dF50PwWauJZ"
   },
   "source": [
    "# **Prueba Semejanza Bigramas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ii8aOJhxaxXR"
   },
   "outputs": [],
   "source": [
    "def semejanzaBigramas(arrBigramas):\n",
    "  '''\n",
    "  La función de semejanzasBigramas(arrBigramas) tiene como objetivo definir los\n",
    "  vectores necesarios para procesar la semejanza entre cada tweet.\n",
    "  El dato de entrada es una lista de lista de strings y el dato de salida es\n",
    "  una lista de vectores.\n",
    "  Se crea una bagOfWords, la cual contiene los bigramas únicos encontrados entre\n",
    "  todos los tweets, después procede a comparar estos con cada uno de los bigramas\n",
    "  de cada tweet, agrega un 1 si ha sido igual y 0 de lo contrario.\n",
    "  '''\n",
    "  bagOfWords = [];\n",
    "\n",
    "  for elem in arrBigramas:\n",
    "    for bigrama in elem:\n",
    "      if bigrama not in bagOfWords:\n",
    "        bagOfWords.append(bigrama)\n",
    "\n",
    "  vectores = []\n",
    "\n",
    "  for elem in arrBigramas:\n",
    "    vector = []\n",
    "    for bigrama in bagOfWords:\n",
    "      if bigrama in elem:\n",
    "        vector.append(1) #Lo encontró\n",
    "      else:\n",
    "        vector.append(0)\n",
    "    vectores.append(vector)#No\n",
    "\n",
    "  return vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3iex26gaydQ"
   },
   "source": [
    "Casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guwfNwfBa-vx",
    "outputId": "af83e191-753f-45ab-c987-fd06daabd27a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
      "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
      "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
      "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
      "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
      "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
      "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
      "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
      "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
      "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
      "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
      "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
      "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
      "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
      "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
      "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
      "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
      "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
      "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
      "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
      "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
      "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
      "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
      "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
      "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 25 tests in 4.600s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x78468508f110>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestSemejanzaBigramas(unittest.TestCase):\n",
    "\n",
    "    def test_bigramas_no_repetidos(self):\n",
    "    #Que los bigramas únicos de múltiples tweets se reflejen correctamente en el vector\n",
    "        entrada = [\n",
    "            [\"ana odia\", \"odia comer\"],\n",
    "            [\"comer manzana\"],\n",
    "            [\"odio mi cuerpo\"]\n",
    "        ]\n",
    "\n",
    "        esperado = [\n",
    "            [1, 1, 0, 0],\n",
    "            [0, 0, 1, 0],\n",
    "            [0, 0, 0, 1]\n",
    "        ]\n",
    "        resultado = semejanzaBigramas(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_bigramas_repetidos(self):\n",
    "    #Que bigramas repetidos en varios tweets se reconozcan como presentes en todos los vectores correspondientes\n",
    "        entrada = [\n",
    "            [\"me gusta\", \"comer sano\"],\n",
    "            [\"comer sano\", \"me gusta\"],\n",
    "            [\"comer sano\"]\n",
    "        ]\n",
    "\n",
    "        esperado = [\n",
    "            [1, 1],\n",
    "            [1, 1],\n",
    "            [0, 1]\n",
    "        ]\n",
    "        resultado = semejanzaBigramas(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_bigramas_unicos(self):\n",
    "    #Que cada tweet con bigrama único tenga exactamente un 1 en su vector en la posición correspondiente\n",
    "        entrada = [\n",
    "            [\"proana es\"],\n",
    "            [\"odio comer\"],\n",
    "            [\"vomité todo\"]\n",
    "        ]\n",
    "\n",
    "        esperado = [\n",
    "            [1, 0, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 0, 1]\n",
    "        ]\n",
    "        resultado = semejanzaBigramas(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_vacio(self):\n",
    "    #Que una entrada vacía no genere errores y devuelva una lista vacía\n",
    "        entrada = [[]]\n",
    "        esperado = [[ ]]\n",
    "        resultado = semejanzaBigramas(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "unittest.main(argv=[''], exit=False, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkpE-2VB5cQD"
   },
   "source": [
    "# **Prueba Sumar semejanzas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CobZz2KF5p7L"
   },
   "outputs": [],
   "source": [
    "def sumar_semejanza(arrVecSemejanza):\n",
    "  '''\n",
    "  La función de sumar_semejanza(arrVecSemejanza), tiene como objetivo conocer el\n",
    "  total de semejanza de coseno, por cada tweet.\n",
    "  El dato de entrada es una lista de vectores y el dato de salida es una lista de\n",
    "  valores, cada valor es la suma de cada tweet.\n",
    "  Por cada valor encontrado en cada vector su suman todos sus valores para\n",
    "  tener solo un valor.\n",
    "  '''\n",
    "  arr = []\n",
    "  val = 0\n",
    "  for n in arrVecSemejanza:\n",
    "    for j in n:\n",
    "      val += j\n",
    "    arr.append(val * 0.1)\n",
    "    val = 0\n",
    "  return arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDZ32cp46C-7"
   },
   "source": [
    "Casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ohfJiDS06CTT",
    "outputId": "44b8af24-ae95-42be-81a4-76f20f9b0695"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
      "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
      "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
      "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
      "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
      "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
      "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
      "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
      "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
      "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
      "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
      "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
      "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
      "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
      "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
      "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
      "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
      "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
      "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
      "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
      "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
      "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
      "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
      "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
      "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
      "test_decimales (__main__.TestSumarSemejanza.test_decimales) ... ok\n",
      "test_lista_vacia (__main__.TestSumarSemejanza.test_lista_vacia) ... ok\n",
      "test_multiples_vectores (__main__.TestSumarSemejanza.test_multiples_vectores) ... ok\n",
      "test_vector_simple (__main__.TestSumarSemejanza.test_vector_simple) ... ok\n",
      "test_vector_vacio (__main__.TestSumarSemejanza.test_vector_vacio) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 30 tests in 4.648s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x78468508abd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestSumarSemejanza(unittest.TestCase):\n",
    "\n",
    "    def test_vector_simple(self):\n",
    "    #Que se sumen correctamente los valores binarios de un solo tweet.\n",
    "        entrada = [[1, 0, 1, 1]]\n",
    "        esperado = [0.3]\n",
    "        resultado = sumar_semejanza(entrada)\n",
    "        self.assertEqual([round(x, 5) for x in resultado], esperado)\n",
    "\n",
    "    def test_multiples_vectores(self):\n",
    "    #Que varios vectores sean procesados individualmente y se retorne una lista con los resultados esperados.\n",
    "        entrada = [\n",
    "            [1, 1],\n",
    "            [0, 0],\n",
    "            [1, 0, 1]\n",
    "        ]\n",
    "        esperado = [0.2, 0.0, 0.2]\n",
    "        resultado = sumar_semejanza(entrada)\n",
    "        self.assertEqual([round(x, 5) for x in resultado], esperado)\n",
    "\n",
    "    def test_vector_vacio(self):\n",
    "    #Que un vector vacío dentro de la lista no genere error y regrese 0.0.\n",
    "        entrada = [[]]\n",
    "        esperado = [0.0]\n",
    "        resultado = sumar_semejanza(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_lista_vacia(self):\n",
    "    #Que una lista completamente vacía regrese una lista vacía.\n",
    "        entrada = []\n",
    "        esperado = []\n",
    "        resultado = sumar_semejanza(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_decimales(self):\n",
    "    #Que se puedan procesar valores decimales, no solo enteros.\n",
    "        entrada = [[0.5, 0.5]]\n",
    "        esperado = [0.1]\n",
    "        resultado = sumar_semejanza(entrada)\n",
    "        self.assertEqual([round(x, 5) for x in resultado], esperado)\n",
    "\n",
    "unittest.main(argv=[''], exit=False, verbosity=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTZFZBItdSQv"
   },
   "source": [
    "# **Prueba Frecuencia de palabras Clave desde una fuente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHbnncuzdXaH"
   },
   "outputs": [],
   "source": [
    "def palabrasClaveFuente(datosLimpios):\n",
    "  '''\n",
    "  La función de palabrasClaveFunte(datosLimpio) tiene como objetivo contabilizar\n",
    "  las palabras clave relacionadas con el trastorno alimenticio de la anorexia.\n",
    "  El dato de entrada es una lista de lista de strings y el dato de salida es un\n",
    "  diccionario con las palabras clave y su conteo.\n",
    "  La definción de estas palabras proviene de la siguiente fuente:\n",
    "  http://ilitia.cua.uam.mx:8080/jspui/handle/123456789/884\n",
    "  '''\n",
    "  palabrasFrecuentes = {'yo':0, 'nosotros':0, 'social':0, 'amigo':0, 'amigos':0, 'amiga':0, 'humano':0, 'ansiedad':0, 'salud':0, 'sentir':0, 'tristeza':0, 'vida':0, 'sexual':0, 'casa':0, 'hogar':0}\n",
    "\n",
    "  for tweet in datosLimpios:\n",
    "    for palabra in tweet:\n",
    "      if palabra in palabrasFrecuentes:\n",
    "        palabrasFrecuentes[palabra] += 1;\n",
    "\n",
    "  return palabrasFrecuentes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kj-0iMD-daX2"
   },
   "source": [
    "Casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4nucetRtdZrm",
    "outputId": "34986767-36bc-4d97-eea3-9b4a24b9b0d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
      "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
      "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
      "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
      "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
      "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
      "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
      "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
      "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
      "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
      "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
      "test_con_multiples_tweets (__main__.TestPalabrasClaveFuente.test_con_multiples_tweets) ... ok\n",
      "test_con_un_tweet (__main__.TestPalabrasClaveFuente.test_con_un_tweet) ... ok\n",
      "test_sin_palabras_clave (__main__.TestPalabrasClaveFuente.test_sin_palabras_clave) ... ok\n",
      "test_tweet_vacio (__main__.TestPalabrasClaveFuente.test_tweet_vacio) ... ok\n",
      "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
      "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
      "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
      "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
      "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
      "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
      "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
      "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
      "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
      "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
      "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
      "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
      "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
      "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
      "test_decimales (__main__.TestSumarSemejanza.test_decimales) ... ok\n",
      "test_lista_vacia (__main__.TestSumarSemejanza.test_lista_vacia) ... ok\n",
      "test_multiples_vectores (__main__.TestSumarSemejanza.test_multiples_vectores) ... ok\n",
      "test_vector_simple (__main__.TestSumarSemejanza.test_vector_simple) ... ok\n",
      "test_vector_vacio (__main__.TestSumarSemejanza.test_vector_vacio) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 34 tests in 4.605s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7846850ac910>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestPalabrasClaveFuente(unittest.TestCase):\n",
    "\n",
    "    def test_con_un_tweet(self):\n",
    "    #Que la función identifique correctamente palabras clave en un solo tweet\n",
    "        entrada = [[\"yo\", \"siento\", \"tristeza\", \"en\", \"mi\", \"hogar\"]]\n",
    "        resultado = palabrasClaveFuente(entrada)\n",
    "\n",
    "        esperado = {\n",
    "            'yo': 1, 'nosotros': 0, 'social': 0, 'amigo': 0,\n",
    "            'amigos': 0, 'amiga': 0, 'humano': 0, 'ansiedad': 0,\n",
    "            'salud': 0, 'sentir': 0, 'tristeza': 1, 'vida': 0,\n",
    "            'sexual': 0, 'casa': 0, 'hogar': 1\n",
    "        }\n",
    "\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_con_multiples_tweets(self):\n",
    "    #Que el conteo se acumule correctamente entre varios tweets\n",
    "        entrada = [\n",
    "            [\"yo\", \"amigos\", \"vida\", \"salud\"],\n",
    "            [\"hogar\", \"sexual\", \"yo\", \"sentir\"],\n",
    "            [\"tristeza\", \"ansiedad\", \"amiga\"]\n",
    "        ]\n",
    "        resultado = palabrasClaveFuente(entrada)\n",
    "\n",
    "        esperado = {\n",
    "            'yo': 2, 'nosotros': 0, 'social': 0, 'amigo': 0,\n",
    "            'amigos': 1, 'amiga': 1, 'humano': 0, 'ansiedad': 1,\n",
    "            'salud': 1, 'sentir': 1, 'tristeza': 1, 'vida': 1,\n",
    "            'sexual': 1, 'casa': 0, 'hogar': 1\n",
    "        }\n",
    "\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_sin_palabras_clave(self):\n",
    "    #Que el resultado sea cero en todas las claves si no hay palabras relevantes según la fuente\n",
    "        entrada = [[\"pizza\", \"felicidad\", \"caminar\"]]\n",
    "        resultado = palabrasClaveFuente(entrada)\n",
    "\n",
    "        esperado = {\n",
    "            'yo': 0, 'nosotros': 0, 'social': 0, 'amigo': 0,\n",
    "            'amigos': 0, 'amiga': 0, 'humano': 0, 'ansiedad': 0,\n",
    "            'salud': 0, 'sentir': 0, 'tristeza': 0, 'vida': 0,\n",
    "            'sexual': 0, 'casa': 0, 'hogar': 0\n",
    "        }\n",
    "\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_tweet_vacio(self):\n",
    "    #Que una entrada vacía no genere errores y todos los conteos sean cero\n",
    "        entrada = [[]]\n",
    "        resultado = palabrasClaveFuente(entrada)\n",
    "\n",
    "        esperado = {\n",
    "            'yo': 0, 'nosotros': 0, 'social': 0, 'amigo': 0,\n",
    "            'amigos': 0, 'amiga': 0, 'humano': 0, 'ansiedad': 0,\n",
    "            'salud': 0, 'sentir': 0, 'tristeza': 0, 'vida': 0,\n",
    "            'sexual': 0, 'casa': 0, 'hogar': 0\n",
    "        }\n",
    "\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "unittest.main(argv=[''], exit=False, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fV4uK-Rf-Pd"
   },
   "source": [
    "# **Prueba - Palabras más frecuentes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzdSfR92gAHM"
   },
   "outputs": [],
   "source": [
    "def palabrasClave(datosLimpio):\n",
    "  '''\n",
    "  La función de palabrasClave(datosLimpio) tiene como objetivo contabilizar\n",
    "  las palabras clave relacionadas con el trastorno alimenticio de la anorexia,\n",
    "  de acuerdo a la información de los tweets ya preprocesaodos.\n",
    "  El dato de entrada es una lista de lista de strings y el dato de salida es un\n",
    "  diccionario con las palabras clave y su conteo\n",
    "  Se revisa cada tweet y las palabras encontradas con el fin de conocer aquellas\n",
    "  palabras con mayor recurrencia en los tweets.\n",
    "  '''\n",
    "\n",
    "  vocabulary = {}\n",
    "  for tweet in datosLimpio:\n",
    "    for palabra in tweet:\n",
    "      if palabra in vocabulary:\n",
    "        vocabulary[palabra] += 1;\n",
    "      else:\n",
    "        vocabulary[palabra] = 1;\n",
    "\n",
    "  relevantes  = sorted(vocabulary.items(), key=lambda item: item[1], reverse=True)[:10]\n",
    "  return relevantes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qVV1jWphNAM"
   },
   "source": [
    "Casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "El88Vc8_hOiU",
    "outputId": "4b46119f-c78a-4dbf-bdf9-0d442c98b4f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
      "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
      "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
      "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
      "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
      "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
      "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
      "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
      "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
      "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
      "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
      "test_caso_comun (__main__.TestPalabrasClave.test_caso_comun) ... ok\n",
      "test_empate_en_frecuencia (__main__.TestPalabrasClave.test_empate_en_frecuencia) ... ok\n",
      "test_menos_de_diez_palabras (__main__.TestPalabrasClave.test_menos_de_diez_palabras) ... ok\n",
      "test_todo_repetido (__main__.TestPalabrasClave.test_todo_repetido) ... ok\n",
      "test_tweet_vacio (__main__.TestPalabrasClave.test_tweet_vacio) ... ok\n",
      "test_con_multiples_tweets (__main__.TestPalabrasClaveFuente.test_con_multiples_tweets) ... ok\n",
      "test_con_un_tweet (__main__.TestPalabrasClaveFuente.test_con_un_tweet) ... ok\n",
      "test_sin_palabras_clave (__main__.TestPalabrasClaveFuente.test_sin_palabras_clave) ... ok\n",
      "test_tweet_vacio (__main__.TestPalabrasClaveFuente.test_tweet_vacio) ... ok\n",
      "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
      "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
      "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
      "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
      "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
      "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
      "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
      "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
      "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
      "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
      "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
      "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
      "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
      "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
      "test_decimales (__main__.TestSumarSemejanza.test_decimales) ... ok\n",
      "test_lista_vacia (__main__.TestSumarSemejanza.test_lista_vacia) ... ok\n",
      "test_multiples_vectores (__main__.TestSumarSemejanza.test_multiples_vectores) ... ok\n",
      "test_vector_simple (__main__.TestSumarSemejanza.test_vector_simple) ... ok\n",
      "test_vector_vacio (__main__.TestSumarSemejanza.test_vector_vacio) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 39 tests in 4.422s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7846850a2810>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestPalabrasClave(unittest.TestCase):\n",
    "\n",
    "    def test_caso_comun(self):\n",
    "    #Que se ordenen correctamente las palabras por frecuencia descendente\n",
    "        entrada = [\n",
    "            [\"ana\", \"comer\", \"vida\"],\n",
    "            [\"ana\", \"comer\", \"feliz\"],\n",
    "            [\"comer\", \"vida\", \"ana\"]\n",
    "        ]\n",
    "        esperado = [(\"ana\", 3), (\"comer\", 3), (\"vida\", 2), (\"feliz\", 1)]\n",
    "        resultado = palabrasClave(entrada)\n",
    "        self.assertEqual(resultado[:4], esperado)\n",
    "\n",
    "    def test_empate_en_frecuencia(self):\n",
    "    #Que en caso de empate, se conserven todas las palabras con conteo igual\n",
    "        entrada = [\n",
    "            [\"ana\", \"comer\"],\n",
    "            [\"vida\", \"comer\"],\n",
    "            [\"ana\", \"vida\"]\n",
    "        ]\n",
    "        esperado = [(\"ana\", 2), (\"comer\", 2), (\"vida\", 2)]\n",
    "        resultado = palabrasClave(entrada)\n",
    "        self.assertEqual(resultado[:3], esperado)\n",
    "\n",
    "    def test_menos_de_diez_palabras(self):\n",
    "    #Que se devuelvan solo las palabras disponibles si hay menos de diez\n",
    "        entrada = [\n",
    "            [\"una\", \"dos\", \"tres\"],\n",
    "            [\"cuatro\", \"cinco\"]\n",
    "        ]\n",
    "        esperado = [\n",
    "            (\"una\", 1), (\"dos\", 1), (\"tres\", 1),\n",
    "            (\"cuatro\", 1), (\"cinco\", 1)\n",
    "        ]\n",
    "        resultado = palabrasClave(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_tweet_vacio(self):\n",
    "    #Que un tweet vacío devuelva una lista vacía como resultado\n",
    "        entrada = [[]]\n",
    "        esperado = []\n",
    "        resultado = palabrasClave(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_todo_repetido(self):\n",
    "    #Que se contabilicen correctamente palabras repetidas\n",
    "        entrada = [\n",
    "            [\"ana\", \"ana\", \"ana\"],\n",
    "            [\"ana\", \"ana\"]\n",
    "        ]\n",
    "        esperado = [(\"ana\", 5)]\n",
    "        resultado = palabrasClave(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "unittest.main(argv=[''], exit=False, verbosity=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1emn--_jjna"
   },
   "source": [
    "# **Prueba TD-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GbVXRj7ajmgK"
   },
   "outputs": [],
   "source": [
    "def calculoTDIDF(resultadoLimpio):\n",
    "  '''\n",
    "   La función de calculoTDIDF(resultadoLimpio) tiene como objetivo identificar\n",
    "   las palabras clave dentro de todo el conjunto de datos y el peso de estas.\n",
    "   El dato de entrada es una lista de lista de strings y el dato de salida es un\n",
    "   dataframe con las palabras clave y su peso correspondiente en cada tweet.\n",
    "   Se busca en tener una oracion con cada tweet y después se procede a realizar\n",
    "   el calculo de TF-IDF. Con el fin de tener información más legible se crea\n",
    "   un dataframe. Y se regresan unicamente las 10 palabras con mayor peso.\n",
    "  '''\n",
    "  #Obtener una oracion por cada tweet\n",
    "  oraciones = [' '.join(palabras) for palabras in resultadoLimpio]\n",
    "\n",
    "  vectorizer = TfidfVectorizer()\n",
    "\n",
    "  # Ajustar datos\n",
    "  matriz = vectorizer.fit_transform(oraciones)\n",
    "\n",
    "  # Matriz legible\n",
    "  df = pd.DataFrame(matriz.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "  tfidf_sums = df.sum(axis=0)\n",
    "\n",
    "  # Primeras 10 palabras  de TF-IDF\n",
    "  claves = tfidf_sums.sort_values(ascending=False).head(10)\n",
    "\n",
    "  return claves\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeKRBRLdjqZx"
   },
   "source": [
    "Casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lu_HVt0MjqMr",
    "outputId": "884505ff-6ebc-45ee-a3ff-f15829014c3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_repite_palabra_en_un_tweet (__main__.TestCalculoTFIDF_Simple.test_repite_palabra_en_un_tweet) ... ok\n",
      "test_tres_tweets_distintos (__main__.TestCalculoTFIDF_Simple.test_tres_tweets_distintos) ... ok\n",
      "test_una_palabra_por_tweet (__main__.TestCalculoTFIDF_Simple.test_una_palabra_por_tweet) ... ok\n",
      "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
      "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
      "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
      "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
      "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
      "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
      "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
      "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
      "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
      "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
      "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
      "test_caso_comun (__main__.TestPalabrasClave.test_caso_comun) ... ok\n",
      "test_empate_en_frecuencia (__main__.TestPalabrasClave.test_empate_en_frecuencia) ... ok\n",
      "test_menos_de_diez_palabras (__main__.TestPalabrasClave.test_menos_de_diez_palabras) ... ok\n",
      "test_todo_repetido (__main__.TestPalabrasClave.test_todo_repetido) ... ok\n",
      "test_tweet_vacio (__main__.TestPalabrasClave.test_tweet_vacio) ... ok\n",
      "test_con_multiples_tweets (__main__.TestPalabrasClaveFuente.test_con_multiples_tweets) ... ok\n",
      "test_con_un_tweet (__main__.TestPalabrasClaveFuente.test_con_un_tweet) ... ok\n",
      "test_sin_palabras_clave (__main__.TestPalabrasClaveFuente.test_sin_palabras_clave) ... ok\n",
      "test_tweet_vacio (__main__.TestPalabrasClaveFuente.test_tweet_vacio) ... ok\n",
      "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
      "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
      "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
      "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
      "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
      "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
      "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
      "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
      "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
      "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
      "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
      "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
      "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
      "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
      "test_decimales (__main__.TestSumarSemejanza.test_decimales) ... ok\n",
      "test_lista_vacia (__main__.TestSumarSemejanza.test_lista_vacia) ... ok\n",
      "test_multiples_vectores (__main__.TestSumarSemejanza.test_multiples_vectores) ... ok\n",
      "test_vector_simple (__main__.TestSumarSemejanza.test_vector_simple) ... ok\n",
      "test_vector_vacio (__main__.TestSumarSemejanza.test_vector_vacio) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 42 tests in 4.167s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7846851e2bd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestCalculoTFIDF_Simple(unittest.TestCase):\n",
    "\n",
    "    def test_tres_tweets_distintos(self):\n",
    "    #Que se generen correctamente las 10 palabras clave de mayor peso\n",
    "      entrada = [\n",
    "          [\"ana\", \"odia\", \"comer\"],\n",
    "          [\"comer\", \"manzana\"],\n",
    "          [\"verde\", \"manzana\"]\n",
    "      ]\n",
    "      resultado = calculoTDIDF(entrada)\n",
    "      self.assertTrue(len(resultado) <= 10)\n",
    "      self.assertIn(\"comer\", resultado.index)\n",
    "      self.assertIn(\"manzana\", resultado.index)\n",
    "\n",
    "\n",
    "    def test_repite_palabra_en_un_tweet(self):\n",
    "    #Que una palabra muy repetida tenga el mayor peso TF-IDF\n",
    "        entrada = [\n",
    "            [\"comer\", \"comer\", \"comer\"],\n",
    "            [\"sano\"],\n",
    "            [\"vida\"]\n",
    "        ]\n",
    "        resultado = calculoTDIDF(entrada)\n",
    "        palabra_mayor_peso = resultado.index[0]\n",
    "        self.assertEqual(palabra_mayor_peso, \"comer\")\n",
    "\n",
    "    def test_una_palabra_por_tweet(self):\n",
    "    #Que se conserven todas las palabras si hay menos de 10, sin errores\n",
    "        entrada = [\n",
    "            [\"comer\"],\n",
    "            [\"vida\"],\n",
    "            [\"ana\"]\n",
    "        ]\n",
    "        resultado = calculoTDIDF(entrada)\n",
    "        self.assertEqual(set(resultado.index), {\"comer\", \"vida\", \"ana\"})\n",
    "\n",
    "unittest.main(argv=[''], exit=False, verbosity=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWF74LDFGnhr"
   },
   "source": [
    "# **Prueba Asiganción palabras**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIbNzsKrGupF"
   },
   "outputs": [],
   "source": [
    "def asignacion_palabra(texto):\n",
    "  '''\n",
    "  La función de asignacion_palabra(texto), tiene como objetivo asignar un valor\n",
    "  a cada palabra que tiene una alta relación con los trastornos alimenticios.\n",
    "  El dato de entrada es una lista de lista de strings y el dato de salida es un\n",
    "  diccionario con las palabras clave y su conteo.\n",
    "  Por cada valor encontrado en cada tweet se suma 0.5 para indicar su relevancia\n",
    "  dentro de todo el conjunto de datos.\n",
    "  '''\n",
    "  encontradas = {\n",
    "      'anorexia': 0,\n",
    "      'bulimia': 0,\n",
    "      'gordo': 0,\n",
    "      'feo': 0,\n",
    "      'ejercicio': 0,\n",
    "      'sin': 0,\n",
    "      'comida': 0,\n",
    "      'bajar': 0,\n",
    "      'peso': 0,\n",
    "      'pastilla': 0,\n",
    "      'día': 0,\n",
    "      'poder': 0,\n",
    "      'querer': 0\n",
    "  } # Se inicia en 0 el dicc\n",
    "  for palabra in texto:\n",
    "    for pal in palabra:\n",
    "      if pal in encontradas: # Revisa\n",
    "        encontradas[pal] += 0.5\n",
    "  return encontradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_-6xnB9G-BU"
   },
   "source": [
    "Casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFZPOb0HHBJL",
    "outputId": "09b748c4-3f32-4ded-f746-bdf12a34ca58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_lista_vacia (__main__.TestAsignacionPalabra.test_lista_vacia) ... ok\n",
      "test_multiples_apariciones (__main__.TestAsignacionPalabra.test_multiples_apariciones) ... ok\n",
      "test_palabras_no_relevantes (__main__.TestAsignacionPalabra.test_palabras_no_relevantes) ... ok\n",
      "test_una_palabra_clave (__main__.TestAsignacionPalabra.test_una_palabra_clave) ... ok\n",
      "test_varias_palabras_dispersas (__main__.TestAsignacionPalabra.test_varias_palabras_dispersas) ... ok\n",
      "test_repite_palabra_en_un_tweet (__main__.TestCalculoTFIDF_Simple.test_repite_palabra_en_un_tweet) ... ok\n",
      "test_tres_tweets_distintos (__main__.TestCalculoTFIDF_Simple.test_tres_tweets_distintos) ... ok\n",
      "test_una_palabra_por_tweet (__main__.TestCalculoTFIDF_Simple.test_una_palabra_por_tweet) ... ok\n",
      "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
      "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
      "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
      "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
      "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
      "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
      "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
      "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
      "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
      "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
      "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
      "test_caso_comun (__main__.TestPalabrasClave.test_caso_comun) ... ok\n",
      "test_empate_en_frecuencia (__main__.TestPalabrasClave.test_empate_en_frecuencia) ... ok\n",
      "test_menos_de_diez_palabras (__main__.TestPalabrasClave.test_menos_de_diez_palabras) ... ok\n",
      "test_todo_repetido (__main__.TestPalabrasClave.test_todo_repetido) ... ok\n",
      "test_tweet_vacio (__main__.TestPalabrasClave.test_tweet_vacio) ... ok\n",
      "test_con_multiples_tweets (__main__.TestPalabrasClaveFuente.test_con_multiples_tweets) ... ok\n",
      "test_con_un_tweet (__main__.TestPalabrasClaveFuente.test_con_un_tweet) ... ok\n",
      "test_sin_palabras_clave (__main__.TestPalabrasClaveFuente.test_sin_palabras_clave) ... ok\n",
      "test_tweet_vacio (__main__.TestPalabrasClaveFuente.test_tweet_vacio) ... ok\n",
      "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
      "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
      "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
      "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
      "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
      "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
      "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
      "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
      "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
      "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
      "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
      "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
      "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
      "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
      "test_decimales (__main__.TestSumarSemejanza.test_decimales) ... ok\n",
      "test_lista_vacia (__main__.TestSumarSemejanza.test_lista_vacia) ... ok\n",
      "test_multiples_vectores (__main__.TestSumarSemejanza.test_multiples_vectores) ... ok\n",
      "test_vector_simple (__main__.TestSumarSemejanza.test_vector_simple) ... ok\n",
      "test_vector_vacio (__main__.TestSumarSemejanza.test_vector_vacio) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 47 tests in 4.982s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7846850f49d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestAsignacionPalabra(unittest.TestCase):\n",
    "\n",
    "    def test_una_palabra_clave(self):\n",
    "    #Que una palabra clave aislada se reconozca correctamente con valor 0.5\n",
    "        entrada = [[\"anorexia\"]]\n",
    "        esperado = {'anorexia': 0.5, 'bulimia': 0, 'gordo': 0, 'feo': 0,\n",
    "                    'ejercicio': 0, 'sin': 0, 'comida': 0, 'bajar': 0,\n",
    "                    'peso': 0, 'pastilla': 0, 'día': 0, 'poder': 0, 'querer': 0}\n",
    "        resultado = asignacion_palabra(entrada)\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_multiples_apariciones(self):\n",
    "    #Que una misma palabra clave que aparece varias veces acumule su peso correctamente\n",
    "        entrada = [[\"anorexia\", \"anorexia\", \"peso\"]]\n",
    "        resultado = asignacion_palabra(entrada)\n",
    "        self.assertEqual(resultado[\"anorexia\"], 1.0)\n",
    "        self.assertEqual(resultado[\"peso\"], 0.5)\n",
    "\n",
    "    def test_varias_palabras_dispersas(self):\n",
    "    #Que diferentes palabras clave distribuidas en varios tweets sean detectadas\n",
    "        entrada = [[\"peso\", \"comida\"], [\"pastilla\", \"día\"], [\"querer\", \"poder\"]]\n",
    "        resultado = asignacion_palabra(entrada)\n",
    "        esperadas = {\"peso\": 0.5, \"comida\": 0.5, \"pastilla\": 0.5, \"día\": 0.5, \"querer\": 0.5, \"poder\": 0.5}\n",
    "        for palabra, valor in esperadas.items():\n",
    "            self.assertEqual(resultado[palabra], valor)\n",
    "\n",
    "    def test_palabras_no_relevantes(self):\n",
    "    #Que palabras no incluidas en el diccionario no afecten el resultado\n",
    "        entrada = [[\"feliz\", \"vida\", \"sol\"]]\n",
    "        resultado = asignacion_palabra(entrada)\n",
    "        for clave in resultado:\n",
    "            self.assertEqual(resultado[clave], 0)\n",
    "\n",
    "    def test_lista_vacia(self):\n",
    "    #Que una entrada vacía no provoque errores y regrese ceros en todas las claves\n",
    "        entrada = [[]]\n",
    "        resultado = asignacion_palabra(entrada)\n",
    "        for clave in resultado:\n",
    "            self.assertEqual(resultado[clave], 0)\n",
    "\n",
    "unittest.main(argv=[''], exit=False, verbosity=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPYSkKauH4C7"
   },
   "source": [
    "# **Prueba Mega Suma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRPfldC9H7ID"
   },
   "outputs": [],
   "source": [
    "def la_megasuma(sumaSmejanza, palabrasClaveDef, resTDIDF, resultado_asignacion, resultadoLimpio):\n",
    "  \"\"\"\n",
    "  La función de la_megasuma tiene como objetivo construir el vector de cada\n",
    "  tweet con base a las diferentes caracteríticas numéricas y cualitativas\n",
    "  encontradaS.\n",
    "  Los datos de entrada son las siguientes:\n",
    "    sumaSmejanza: Lista de bigramas.\n",
    "    palabrasClaveDef: Diccionario de la palabra y su peso.\n",
    "    resTDIDF: Una serie de pandas con el valor TF-IDF.\n",
    "    resultado_asignacion: Diccionario de los palabras clave y su peso.\n",
    "    resultadoLimpio:Lista de lista con strings.\n",
    "  Regresa una lista de lista con 4 valores por cada tweet.\n",
    "  \"\"\"\n",
    "  sumaTotal = []\n",
    "  valorTweet =[]\n",
    "\n",
    "  palabrasClave_dict = dict(palabrasClaveDef)\n",
    "\n",
    "  for i, tweet_words in enumerate(resultadoLimpio):\n",
    "    current_tweet_sum = 0\n",
    "    conteo_palClave = 0\n",
    "    conteo_TDIDF = 0\n",
    "    conteo_asignacion = 0\n",
    "\n",
    "    #Empeieza con los bigramas\n",
    "    if i < len(sumaSmejanza):\n",
    "        valorTweet.append(sumaSmejanza[i])\n",
    "    else:\n",
    "        print(f\"Warning: Index {i} out of bounds for sumaSmejanza.\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    for word in tweet_words:\n",
    "      # palabrasClave\n",
    "      if word in palabrasClave_dict:\n",
    "        conteo_palClave += palabrasClave_dict[word]\n",
    "\n",
    "      # TDIDF\n",
    "      if word in resTDIDF.index:\n",
    "        conteo_TDIDF += resTDIDF[word]\n",
    "\n",
    "      # palabrasImportantes\n",
    "      if word in resultado_asignacion:\n",
    "        conteo_asignacion += resultado_asignacion[word]\n",
    "\n",
    "\n",
    "    valorTweet.append(conteo_palClave)\n",
    "    valorTweet.append(conteo_TDIDF)\n",
    "    valorTweet.append(conteo_asignacion)\n",
    "    sumaTotal.append(valorTweet)\n",
    "    valorTweet = []\n",
    "\n",
    "  return sumaTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2TXXnO6ZLNMq",
    "outputId": "b9ae14a8-7054-4c05-cd2f-45369e6a26fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_lista_vacia (__main__.TestAsignacionPalabra.test_lista_vacia) ... ok\n",
      "test_multiples_apariciones (__main__.TestAsignacionPalabra.test_multiples_apariciones) ... ok\n",
      "test_palabras_no_relevantes (__main__.TestAsignacionPalabra.test_palabras_no_relevantes) ... ok\n",
      "test_una_palabra_clave (__main__.TestAsignacionPalabra.test_una_palabra_clave) ... ok\n",
      "test_varias_palabras_dispersas (__main__.TestAsignacionPalabra.test_varias_palabras_dispersas) ... ok\n",
      "test_repite_palabra_en_un_tweet (__main__.TestCalculoTFIDF_Simple.test_repite_palabra_en_un_tweet) ... ok\n",
      "test_tres_tweets_distintos (__main__.TestCalculoTFIDF_Simple.test_tres_tweets_distintos) ... ok\n",
      "test_una_palabra_por_tweet (__main__.TestCalculoTFIDF_Simple.test_una_palabra_por_tweet) ... ok\n",
      "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
      "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
      "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
      "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
      "test_entrada_vacia (__main__.TestLaMegaSuma.test_entrada_vacia) ... ok\n",
      "test_tweet_sin_palabras_clave (__main__.TestLaMegaSuma.test_tweet_sin_palabras_clave) ... ok\n",
      "test_una_entrada_simple (__main__.TestLaMegaSuma.test_una_entrada_simple) ... ok\n",
      "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
      "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
      "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
      "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
      "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
      "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
      "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
      "test_caso_comun (__main__.TestPalabrasClave.test_caso_comun) ... ok\n",
      "test_empate_en_frecuencia (__main__.TestPalabrasClave.test_empate_en_frecuencia) ... ok\n",
      "test_menos_de_diez_palabras (__main__.TestPalabrasClave.test_menos_de_diez_palabras) ... ok\n",
      "test_todo_repetido (__main__.TestPalabrasClave.test_todo_repetido) ... ok\n",
      "test_tweet_vacio (__main__.TestPalabrasClave.test_tweet_vacio) ... ok\n",
      "test_con_multiples_tweets (__main__.TestPalabrasClaveFuente.test_con_multiples_tweets) ... ok\n",
      "test_con_un_tweet (__main__.TestPalabrasClaveFuente.test_con_un_tweet) ... ok\n",
      "test_sin_palabras_clave (__main__.TestPalabrasClaveFuente.test_sin_palabras_clave) ... ok\n",
      "test_tweet_vacio (__main__.TestPalabrasClaveFuente.test_tweet_vacio) ... ok\n",
      "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
      "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
      "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
      "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
      "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
      "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
      "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
      "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
      "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
      "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
      "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
      "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
      "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
      "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
      "test_decimales (__main__.TestSumarSemejanza.test_decimales) ... ok\n",
      "test_lista_vacia (__main__.TestSumarSemejanza.test_lista_vacia) ... ok\n",
      "test_multiples_vectores (__main__.TestSumarSemejanza.test_multiples_vectores) ... ok\n",
      "test_vector_simple (__main__.TestSumarSemejanza.test_vector_simple) ... ok\n",
      "test_vector_vacio (__main__.TestSumarSemejanza.test_vector_vacio) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 50 tests in 4.279s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x78468507b050>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "\n",
    "class TestLaMegaSuma(unittest.TestCase):\n",
    "\n",
    "    def test_una_entrada_simple(self):\n",
    "    #Que se construya correctamente el vector final con las 4 métricas esperadas\n",
    "        sumaSmejanza = [0.2]\n",
    "        palabrasClaveDef = [(\"ana\", 1), (\"vida\", 2)]\n",
    "        resTDIDF = pd.Series({\"ana\": 0.5, \"comer\": 0.1})\n",
    "        resultado_asignacion = {\"ana\": 0.5, \"comer\": 0.2}\n",
    "        resultadoLimpio = [[\"ana\", \"comer\"]]\n",
    "\n",
    "        resultado = la_megasuma(\n",
    "            sumaSmejanza,\n",
    "            palabrasClaveDef,\n",
    "            resTDIDF,\n",
    "            resultado_asignacion,\n",
    "            resultadoLimpio\n",
    "        )\n",
    "\n",
    "        esperado = [[0.2, 1, 0.6, 0.7]]\n",
    "        resultado_redondeado = [round(x, 5) for x in resultado[0]]\n",
    "        esperado_redondeado = [round(x, 5) for x in esperado[0]]\n",
    "\n",
    "        self.assertEqual(resultado_redondeado, esperado_redondeado)\n",
    "\n",
    "    def test_tweet_sin_palabras_clave(self):\n",
    "    #Que tweets sin coincidencias retornen ceros (excepto en bigramas)\n",
    "        sumaSmejanza = [0.0]\n",
    "        palabrasClaveDef = [(\"vida\", 1)]\n",
    "        resTDIDF = pd.Series({\"vida\": 0.2})\n",
    "        resultado_asignacion = {\"vida\": 0.1}\n",
    "        resultadoLimpio = [[\"hola\", \"mundo\"]]\n",
    "\n",
    "        resultado = la_megasuma(\n",
    "            sumaSmejanza,\n",
    "            palabrasClaveDef,\n",
    "            resTDIDF,\n",
    "            resultado_asignacion,\n",
    "            resultadoLimpio\n",
    "        )\n",
    "\n",
    "        esperado = [[0.0, 0, 0, 0]]\n",
    "\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "    def test_entrada_vacia(self):\n",
    "    #Que una entrada completamente vacía devuelva una lista vacía sin errores\n",
    "        sumaSmejanza = []\n",
    "        palabrasClaveDef = []\n",
    "        resTDIDF = pd.Series(dtype=float)\n",
    "        resultado_asignacion = {}\n",
    "        resultadoLimpio = []\n",
    "\n",
    "        resultado = la_megasuma(\n",
    "            sumaSmejanza,\n",
    "            palabrasClaveDef,\n",
    "            resTDIDF,\n",
    "            resultado_asignacion,\n",
    "            resultadoLimpio\n",
    "            )\n",
    "        esperado = []\n",
    "        self.assertEqual(resultado, esperado)\n",
    "\n",
    "unittest.main(argv=[''], exit=False, verbosity=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7B4KAcGvN9o"
   },
   "source": [
    "# **Prueba Vectorización**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNAthZjIvQy3"
   },
   "outputs": [],
   "source": [
    "def vectorizacion(resultadoLimpio):\n",
    "  '''\n",
    "  La función de vectorizacion(resultadoLimpio) tiene como objetivo obtener el\n",
    "  valor vectorial por cada una de las palabaras encontradas en los tweets ya\n",
    "  preprocesados.\n",
    "  El dato de entrada es una lista de lista de strings y el dato de salida es un\n",
    "  objeto de tipo Word2Vec.\n",
    "  '''\n",
    "\n",
    "  model = Word2Vec(\n",
    "    sentences = resultadoLimpio,\n",
    "    vector_size = 70,\n",
    "    sg = 1,\n",
    "    window = 10,\n",
    "    min_count = 1,\n",
    "  )\n",
    "  return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQpfEBg9vn7f"
   },
   "source": [
    "Casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q62Xf343vpLW",
    "outputId": "82f21762-9825-4048-afca-ca2937da8c40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_lista_vacia (__main__.TestAsignacionPalabra.test_lista_vacia) ... ok\n",
      "test_multiples_apariciones (__main__.TestAsignacionPalabra.test_multiples_apariciones) ... ok\n",
      "test_palabras_no_relevantes (__main__.TestAsignacionPalabra.test_palabras_no_relevantes) ... ok\n",
      "test_una_palabra_clave (__main__.TestAsignacionPalabra.test_una_palabra_clave) ... ok\n",
      "test_varias_palabras_dispersas (__main__.TestAsignacionPalabra.test_varias_palabras_dispersas) ... ok\n",
      "test_repite_palabra_en_un_tweet (__main__.TestCalculoTFIDF_Simple.test_repite_palabra_en_un_tweet) ... ok\n",
      "test_tres_tweets_distintos (__main__.TestCalculoTFIDF_Simple.test_tres_tweets_distintos) ... ok\n",
      "test_una_palabra_por_tweet (__main__.TestCalculoTFIDF_Simple.test_una_palabra_por_tweet) ... ok\n",
      "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
      "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
      "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
      "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
      "test_entrada_vacia (__main__.TestLaMegaSuma.test_entrada_vacia) ... ok\n",
      "test_tweet_sin_palabras_clave (__main__.TestLaMegaSuma.test_tweet_sin_palabras_clave) ... ok\n",
      "test_una_entrada_simple (__main__.TestLaMegaSuma.test_una_entrada_simple) ... ok\n",
      "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
      "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
      "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
      "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
      "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
      "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
      "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
      "test_caso_comun (__main__.TestPalabrasClave.test_caso_comun) ... ok\n",
      "test_empate_en_frecuencia (__main__.TestPalabrasClave.test_empate_en_frecuencia) ... ok\n",
      "test_menos_de_diez_palabras (__main__.TestPalabrasClave.test_menos_de_diez_palabras) ... ok\n",
      "test_todo_repetido (__main__.TestPalabrasClave.test_todo_repetido) ... ok\n",
      "test_tweet_vacio (__main__.TestPalabrasClave.test_tweet_vacio) ... ok\n",
      "test_con_multiples_tweets (__main__.TestPalabrasClaveFuente.test_con_multiples_tweets) ... ok\n",
      "test_con_un_tweet (__main__.TestPalabrasClaveFuente.test_con_un_tweet) ... ok\n",
      "test_sin_palabras_clave (__main__.TestPalabrasClaveFuente.test_sin_palabras_clave) ... ok\n",
      "test_tweet_vacio (__main__.TestPalabrasClaveFuente.test_tweet_vacio) ... ok\n",
      "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
      "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
      "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
      "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
      "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
      "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
      "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
      "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
      "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
      "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
      "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
      "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
      "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
      "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
      "test_decimales (__main__.TestSumarSemejanza.test_decimales) ... ok\n",
      "test_lista_vacia (__main__.TestSumarSemejanza.test_lista_vacia) ... ok\n",
      "test_multiples_vectores (__main__.TestSumarSemejanza.test_multiples_vectores) ... ok\n",
      "test_vector_simple (__main__.TestSumarSemejanza.test_vector_simple) ... ok\n",
      "test_vector_vacio (__main__.TestSumarSemejanza.test_vector_vacio) ... ok\n",
      "test_es_un_modelo_word2vec (__main__.TestVectorizacion.test_es_un_modelo_word2vec) ... ok\n",
      "test_modelo_funciona_con_tweet_unico (__main__.TestVectorizacion.test_modelo_funciona_con_tweet_unico) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 52 tests in 4.202s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x784685145f10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestVectorizacion(unittest.TestCase):\n",
    "\n",
    "    def test_es_un_modelo_word2vec(self):\n",
    "    #Que el objeto retornado sea una instancia válida de gensim.models.Word2Vec\n",
    "        entrada = [[\"ana\", \"odia\", \"comer\"], [\"comer\", \"sano\"]]\n",
    "        modelo = vectorizacion(entrada)\n",
    "        self.assertIsInstance(modelo, Word2Vec)\n",
    "\n",
    "    def test_modelo_funciona_con_tweet_unico(self):\n",
    "    #Que incluso con un solo tweet largo, el modelo entrene y contenga todas las palabras\n",
    "        entrada = [[\"comer\", \"sano\", \"vida\", \"feliz\", \"ana\"]]\n",
    "        modelo = vectorizacion(entrada)\n",
    "        for palabra in entrada[0]:\n",
    "            self.assertIn(palabra, modelo.wv.key_to_index)\n",
    "\n",
    "unittest.main(argv=[''], exit=False, verbosity=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tyax_SaLyXDi"
   },
   "source": [
    "# **Prueba Transformación**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2ZRMrbD4opu"
   },
   "source": [
    " Para las siguientes dos pruebas se debe\n",
    "1. Haber definido previamente la función limpiezaAbsoluta(), que limpia los tweets:\n",
    "    - Elimina menciones, hashtags, URLs, puntuación, emojis y stopwords.\n",
    "    - Reemplaza jergas como \"ana\" o \"mía\" por \"anorexia\" o \"bulimia\".\n",
    "    - Lematiza las palabras (reduce a su forma base).\n",
    "\n",
    "2. Haber definido vectorizacion(), que entrena un modelo Word2Vec con los tweets limpios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3BnFo293KAG"
   },
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"Me siento culpable después de comer 😔 @ana_apoyo #culpa\",\n",
    "    \"La delgadez es mi meta, no quiero engordar jamás #thinspo\",\n",
    "    \"Gracias @ana_power por darme fuerza para resistir la comida #proana\",\n",
    "    \"Después de cada atracón, mía me libera... @bulimia_team #purga\",\n",
    "    \"Hoy no comeré nada, solo agua y té 🍵 #fasting #control\"\n",
    "]\n",
    "\n",
    "resultadoLimpio = limpiezaAbsoluta(tweets)\n",
    "modeloVector = vectorizacion(resultadoLimpio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jssXoewyawt"
   },
   "outputs": [],
   "source": [
    "def transformacion(tweets):\n",
    "  '''\n",
    "  La función transformacion(tweets) tiene como objetivo que por cada palabra\n",
    "  encontrada en cada tweet, se tenga un valor vectorial.\n",
    "  El dato de entrada es una lista de strings y el dato de salida es un vector\n",
    "  por cada tweet.\n",
    "  Se busca que por cada tweet tener solo un vector que contenga todos los\n",
    "  valores de cada palabra.\n",
    "  '''\n",
    "  total = 0\n",
    "  for palabra in tweets:\n",
    "    total += modeloVector.wv[palabra]\n",
    "  return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuH_oSN355-h"
   },
   "source": [
    "Casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GkRk3S5r576a",
    "outputId": "8466bcdc-e406-4b85-8fb4-fcaf9a886cab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_lista_vacia (__main__.TestAsignacionPalabra.test_lista_vacia) ... ok\n",
      "test_multiples_apariciones (__main__.TestAsignacionPalabra.test_multiples_apariciones) ... ok\n",
      "test_palabras_no_relevantes (__main__.TestAsignacionPalabra.test_palabras_no_relevantes) ... ok\n",
      "test_una_palabra_clave (__main__.TestAsignacionPalabra.test_una_palabra_clave) ... ok\n",
      "test_varias_palabras_dispersas (__main__.TestAsignacionPalabra.test_varias_palabras_dispersas) ... ok\n",
      "test_repite_palabra_en_un_tweet (__main__.TestCalculoTFIDF_Simple.test_repite_palabra_en_un_tweet) ... ok\n",
      "test_tres_tweets_distintos (__main__.TestCalculoTFIDF_Simple.test_tres_tweets_distintos) ... ok\n",
      "test_una_palabra_por_tweet (__main__.TestCalculoTFIDF_Simple.test_una_palabra_por_tweet) ... ok\n",
      "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
      "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
      "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
      "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
      "test_entrada_vacia (__main__.TestLaMegaSuma.test_entrada_vacia) ... ok\n",
      "test_tweet_sin_palabras_clave (__main__.TestLaMegaSuma.test_tweet_sin_palabras_clave) ... ok\n",
      "test_una_entrada_simple (__main__.TestLaMegaSuma.test_una_entrada_simple) ... ok\n",
      "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
      "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
      "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
      "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
      "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
      "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
      "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
      "test_caso_comun (__main__.TestPalabrasClave.test_caso_comun) ... ok\n",
      "test_empate_en_frecuencia (__main__.TestPalabrasClave.test_empate_en_frecuencia) ... ok\n",
      "test_menos_de_diez_palabras (__main__.TestPalabrasClave.test_menos_de_diez_palabras) ... ok\n",
      "test_todo_repetido (__main__.TestPalabrasClave.test_todo_repetido) ... ok\n",
      "test_tweet_vacio (__main__.TestPalabrasClave.test_tweet_vacio) ... ok\n",
      "test_con_multiples_tweets (__main__.TestPalabrasClaveFuente.test_con_multiples_tweets) ... ok\n",
      "test_con_un_tweet (__main__.TestPalabrasClaveFuente.test_con_un_tweet) ... ok\n",
      "test_sin_palabras_clave (__main__.TestPalabrasClaveFuente.test_sin_palabras_clave) ... ok\n",
      "test_tweet_vacio (__main__.TestPalabrasClaveFuente.test_tweet_vacio) ... ok\n",
      "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
      "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
      "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
      "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
      "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
      "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
      "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
      "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
      "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
      "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
      "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
      "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
      "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
      "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
      "test_decimales (__main__.TestSumarSemejanza.test_decimales) ... ok\n",
      "test_lista_vacia (__main__.TestSumarSemejanza.test_lista_vacia) ... ok\n",
      "test_multiples_vectores (__main__.TestSumarSemejanza.test_multiples_vectores) ... ok\n",
      "test_vector_simple (__main__.TestSumarSemejanza.test_vector_simple) ... ok\n",
      "test_vector_vacio (__main__.TestSumarSemejanza.test_vector_vacio) ... ok\n",
      "test_vector_devuelto_es_array (__main__.TestTransformacionBasica.test_vector_devuelto_es_array) ... ok\n",
      "test_vector_tiene_datos (__main__.TestTransformacionBasica.test_vector_tiene_datos) ... ok\n",
      "test_vector_tweet_vacio (__main__.TestTransformacionBasica.test_vector_tweet_vacio) ... ok\n",
      "test_es_un_modelo_word2vec (__main__.TestVectorizacion.test_es_un_modelo_word2vec) ... ok\n",
      "test_modelo_funciona_con_tweet_unico (__main__.TestVectorizacion.test_modelo_funciona_con_tweet_unico) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 55 tests in 3.948s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7846850cdf10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestTransformacionBasica(unittest.TestCase):\n",
    "\n",
    "    def test_vector_devuelto_es_array(self):\n",
    "    #Que el resultado sea un vector de tipo np.ndarray\n",
    "        tweet = resultadoLimpio[0]\n",
    "        vector = transformacion(tweet)\n",
    "        self.assertIsInstance(vector, np.ndarray)\n",
    "\n",
    "    def test_vector_tiene_datos(self):\n",
    "    #Que el vector no esté vacío ni compuesto solo por ceros\n",
    "        tweet = resultadoLimpio[1]\n",
    "        vector = transformacion(tweet)\n",
    "        self.assertTrue(any(vector))\n",
    "\n",
    "    def test_vector_tweet_vacio(self):\n",
    "    #Que un tweet vacío retorne un vector de ceros\n",
    "        vector = transformacion([])\n",
    "        self.assertTrue(np.all(vector == 0))\n",
    "\n",
    "unittest.main(argv=[''], exit=False, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iqQ_VE4-Aen"
   },
   "source": [
    "# **Prueba Procesamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6B6XGIN-DQ4"
   },
   "outputs": [],
   "source": [
    "def procesamiento(resultadoLimpio):\n",
    "  '''\n",
    "  La función procesamiento(resultadoLimpio) tiene como objetivo descomponer la\n",
    "  lista tweets.\n",
    "  El dato de entrada es una lista de lista de strings y el dato de salida es una\n",
    "  lista de vectores\n",
    "  Se busca que por cada tweet se obtenga su respectivo valor vectorial, respecto\n",
    "  a las palabras que contiene.\n",
    "  '''\n",
    "  numeroTweets = []\n",
    "  for tweet in resultadoLimpio:\n",
    "    numeroTweets.append(transformacion(tweet))\n",
    "  return numeroTweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XTwuAba_Lwf"
   },
   "source": [
    "Casos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjDZY-nc_LDm",
    "outputId": "574f53ab-80c8-401e-da6f-61af2e11a424"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_lista_vacia (__main__.TestAsignacionPalabra.test_lista_vacia) ... ok\n",
      "test_multiples_apariciones (__main__.TestAsignacionPalabra.test_multiples_apariciones) ... ok\n",
      "test_palabras_no_relevantes (__main__.TestAsignacionPalabra.test_palabras_no_relevantes) ... ok\n",
      "test_una_palabra_clave (__main__.TestAsignacionPalabra.test_una_palabra_clave) ... ok\n",
      "test_varias_palabras_dispersas (__main__.TestAsignacionPalabra.test_varias_palabras_dispersas) ... ok\n",
      "test_repite_palabra_en_un_tweet (__main__.TestCalculoTFIDF_Simple.test_repite_palabra_en_un_tweet) ... ok\n",
      "test_tres_tweets_distintos (__main__.TestCalculoTFIDF_Simple.test_tres_tweets_distintos) ... ok\n",
      "test_una_palabra_por_tweet (__main__.TestCalculoTFIDF_Simple.test_una_palabra_por_tweet) ... ok\n",
      "test_mixto_convertidos_y_normales (__main__.TestJergaProceso.test_mixto_convertidos_y_normales) ... ok\n",
      "test_palabras_convertidas_a_anorexia (__main__.TestJergaProceso.test_palabras_convertidas_a_anorexia) ... ok\n",
      "test_palabras_convertidas_a_bulimia (__main__.TestJergaProceso.test_palabras_convertidas_a_bulimia) ... ok\n",
      "test_palabras_no_convertidas (__main__.TestJergaProceso.test_palabras_no_convertidas) ... ok\n",
      "test_entrada_vacia (__main__.TestLaMegaSuma.test_entrada_vacia) ... ok\n",
      "test_tweet_sin_palabras_clave (__main__.TestLaMegaSuma.test_tweet_sin_palabras_clave) ... ok\n",
      "test_una_entrada_simple (__main__.TestLaMegaSuma.test_una_entrada_simple) ... ok\n",
      "test_mixto_regulares_irregulares_neutros (__main__.TestLematizacionPasado.test_mixto_regulares_irregulares_neutros) ... ok\n",
      "test_pasado_irregular (__main__.TestLematizacionPasado.test_pasado_irregular) ... ok\n",
      "test_pasado_regular (__main__.TestLematizacionPasado.test_pasado_regular) ... ok\n",
      "test_tweet1 (__main__.TestLimpiezaAbsoluta.test_tweet1) ... ok\n",
      "test_tweet2 (__main__.TestLimpiezaAbsoluta.test_tweet2) ... ok\n",
      "test_tweet3 (__main__.TestLimpiezaAbsoluta.test_tweet3) ... ok\n",
      "test_tweet4 (__main__.TestLimpiezaAbsoluta.test_tweet4) ... ok\n",
      "test_caso_comun (__main__.TestPalabrasClave.test_caso_comun) ... ok\n",
      "test_empate_en_frecuencia (__main__.TestPalabrasClave.test_empate_en_frecuencia) ... ok\n",
      "test_menos_de_diez_palabras (__main__.TestPalabrasClave.test_menos_de_diez_palabras) ... ok\n",
      "test_todo_repetido (__main__.TestPalabrasClave.test_todo_repetido) ... ok\n",
      "test_tweet_vacio (__main__.TestPalabrasClave.test_tweet_vacio) ... ok\n",
      "test_con_multiples_tweets (__main__.TestPalabrasClaveFuente.test_con_multiples_tweets) ... ok\n",
      "test_con_un_tweet (__main__.TestPalabrasClaveFuente.test_con_un_tweet) ... ok\n",
      "test_sin_palabras_clave (__main__.TestPalabrasClaveFuente.test_sin_palabras_clave) ... ok\n",
      "test_tweet_vacio (__main__.TestPalabrasClaveFuente.test_tweet_vacio) ... ok\n",
      "test_lista_de_vectores (__main__.TestProcesamiento.test_lista_de_vectores) ... ok\n",
      "test_vector_es_ndarray (__main__.TestProcesamiento.test_vector_es_ndarray) ... ok\n",
      "test_vector_tiene_datos (__main__.TestProcesamiento.test_vector_tiene_datos) ... ok\n",
      "test_bigrama_multiple (__main__.TestRealizarBigramasAnorexia.test_bigrama_multiple) ... ok\n",
      "test_bigrama_simple (__main__.TestRealizarBigramasAnorexia.test_bigrama_simple) ... ok\n",
      "test_frase_vacia (__main__.TestRealizarBigramasAnorexia.test_frase_vacia) ... ok\n",
      "test_palabra_unica (__main__.TestRealizarBigramasAnorexia.test_palabra_unica) ... ok\n",
      "test_emojis (__main__.TestReglasLimpieza.test_emojis) ... ok\n",
      "test_espacios_extras (__main__.TestReglasLimpieza.test_espacios_extras) ... ok\n",
      "test_hashtags (__main__.TestReglasLimpieza.test_hashtags) ... ok\n",
      "test_menciones (__main__.TestReglasLimpieza.test_menciones) ... ok\n",
      "test_puntuacion (__main__.TestReglasLimpieza.test_puntuacion) ... ok\n",
      "test_urls (__main__.TestReglasLimpieza.test_urls) ... ok\n",
      "test_bigramas_no_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_no_repetidos) ... ok\n",
      "test_bigramas_repetidos (__main__.TestSemejanzaBigramas.test_bigramas_repetidos) ... ok\n",
      "test_bigramas_unicos (__main__.TestSemejanzaBigramas.test_bigramas_unicos) ... ok\n",
      "test_vacio (__main__.TestSemejanzaBigramas.test_vacio) ... ok\n",
      "test_decimales (__main__.TestSumarSemejanza.test_decimales) ... ok\n",
      "test_lista_vacia (__main__.TestSumarSemejanza.test_lista_vacia) ... ok\n",
      "test_multiples_vectores (__main__.TestSumarSemejanza.test_multiples_vectores) ... ok\n",
      "test_vector_simple (__main__.TestSumarSemejanza.test_vector_simple) ... ok\n",
      "test_vector_vacio (__main__.TestSumarSemejanza.test_vector_vacio) ... ok\n",
      "test_vector_devuelto_es_array (__main__.TestTransformacionBasica.test_vector_devuelto_es_array) ... ok\n",
      "test_vector_tiene_datos (__main__.TestTransformacionBasica.test_vector_tiene_datos) ... ok\n",
      "test_vector_tweet_vacio (__main__.TestTransformacionBasica.test_vector_tweet_vacio) ... ok\n",
      "test_es_un_modelo_word2vec (__main__.TestVectorizacion.test_es_un_modelo_word2vec) ... ok\n",
      "test_modelo_funciona_con_tweet_unico (__main__.TestVectorizacion.test_modelo_funciona_con_tweet_unico) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 58 tests in 3.956s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x784685048f90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestProcesamiento(unittest.TestCase):\n",
    "\n",
    "    def test_lista_de_vectores(self):\n",
    "    #Que se retorne un vector por cada tweet en la entrada\n",
    "        vectores = procesamiento(resultadoLimpio)\n",
    "        self.assertEqual(len(vectores), len(resultadoLimpio))\n",
    "\n",
    "    def test_vector_es_ndarray(self):\n",
    "    #Que cada elemento de la lista de salida sea un np.ndarray válido\n",
    "        vectores = procesamiento(resultadoLimpio)\n",
    "        self.assertIsInstance(vectores[0], np.ndarray)\n",
    "\n",
    "    def test_vector_tiene_datos(self):\n",
    "    #Que al menos un vector contenga valores diferentes de cero\n",
    "        vectores = procesamiento(resultadoLimpio)\n",
    "        self.assertTrue(any(vectores[0]))\n",
    "\n",
    "unittest.main(argv=[''], exit=False, verbosity=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
