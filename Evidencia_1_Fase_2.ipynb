{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ingrid-Garcia/Fase-2-usoNLP/blob/main/Evidencia_1_Fase_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tXgVP7lI0LJ"
      },
      "source": [
        "#Evidencia 1- Fase 2\n",
        "Equipo 5:<br>\n",
        "Ingrid García Hernández, A01754475<br>\n",
        "Abigail Donají Chavez Rubio, A01747423<br>\n",
        "Noh Ah Kim Kwon, A01747512<br>\n",
        "Eduardo Alfredo Ramírez Muñoz, A01754917\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR8Jf3FY1k0d"
      },
      "source": [
        "# Definción de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUzKYyNDzQgl"
      },
      "outputs": [],
      "source": [
        "pip install pandas openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y96DBzm-AW7T"
      },
      "outputs": [],
      "source": [
        "pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWDgD9z2qzhV"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "!pip install --upgrade numpy==1.25.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNdAxqa52y_r"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import brown\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.util import ngrams\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "from sklearn.metrics import pairwise\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "\n",
        "from six import StringIO\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "\n",
        "import pydotplus\n",
        "\n",
        "\n",
        "import stanza\n",
        "nlp = stanza.Pipeline('es')\n",
        "\n",
        "import string\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "from matplotlib import pyplot\n",
        "\n",
        "\n",
        "from sklearn.tree import export_graphviz\n",
        "from six import StringIO\n",
        "from IPython.display import Image\n",
        "import pydotplus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdIUtRC91q32"
      },
      "source": [
        "# Carga de archivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX0Vgesr0n2E"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #Acceso al csv, encontrado en el google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bi7IyzcQ1CQo"
      },
      "outputs": [],
      "source": [
        "ruta = '/content/drive/MyDrive/data_train(in).csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0N43lbU1kEs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(ruta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuBeLSXv2cJ6"
      },
      "outputs": [],
      "source": [
        "#Definicion de variables\n",
        "tweets = df.tweet_text\n",
        "clase = df.iloc[:, 3]\n",
        "tweets = tweets [:1200] #Texto\n",
        "clases = clase [:1200] #1 --> Anorexia, 0 --> No anorexia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_QjiD8v2rGG"
      },
      "source": [
        "# Preprocesamiento del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvAYO_0X13hC"
      },
      "source": [
        "**Uso de expresiones regulares**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mMP-7eh3BdS"
      },
      "outputs": [],
      "source": [
        "def limpiar_texto(texto):\n",
        "    '''\n",
        "    La función de limpiar_texto(texto), tiene como objetivo eliminar el ruido\n",
        "    dentro de cada elemento de la lista.\n",
        "    El dato de entrada es una lista de strings y el dato de salida es\n",
        "    una lista de strings.\n",
        "    Por cada palabra se debe de normalizar, es decir, se hace caso omiso de los\n",
        "    acentos y después se revisa cada palabra con respecto a  las reglas\n",
        "    definidas. Esto se hace por medio de expresiones regulares. Igualmente, se\n",
        "    eliminan las palabras 'innecesarias',dado que no proporcionan mayor\n",
        "    información o contexto.\n",
        "    '''\n",
        "    #texto.lower()\n",
        "    tweet = []\n",
        "    for palabra in texto.split():\n",
        "      nuevo = palabra\n",
        "      #Acentos\n",
        "      nuevo = unicodedata.normalize ('NFKD', nuevo).encode ('ascii', 'ignore').decode ('utf-8', 'ignore')\n",
        "      for i, j in reglas.items():\n",
        "        if i == 'espacios_extra':\n",
        "          nuevo = re.sub(j, ' ', nuevo)\n",
        "        else:\n",
        "          nuevo = re.sub(j, '',nuevo)\n",
        "      if nuevo not in stopwords.words('spanish'):\n",
        "        tweet.append(nuevo.lower())\n",
        "\n",
        "    return tweet\n",
        "\n",
        "reglas = {\n",
        "    'hashtags': r'#',\n",
        "    'menciones': r'@',\n",
        "    'urls': r'http\\S+|www.\\S+',\n",
        "    'emojis': r'[^\\x00-\\x7F]+',\n",
        "    'puntuacion': r'[^\\w\\s]',\n",
        "    'espacios_extras': r'\\s+' #Tabs, dobles espacios\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIcpX-8NQUE4"
      },
      "source": [
        "**Manejo de jerga**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGpzt1Qc2qkL"
      },
      "outputs": [],
      "source": [
        "def jerga_proceso(texto):\n",
        "  '''\n",
        "    La función de jerga(texto), tiene como objetivo proporcionar información más\n",
        "    precisa de palabras relaciondas con este desorden alimenticio, mas no lo\n",
        "    hacen de manera directa.\n",
        "    El dato de entrada es una lista de lista de strings y el dato de salida es\n",
        "    una lista de strings.\n",
        "    Se asuma que la entrada es una lista de palabras y por ende revisa cada una\n",
        "    de estas para revisar que no se haga caso omiso de ningún caso de jerga.\n",
        "    '''\n",
        "  tweet = []\n",
        "  for palabra in texto:\n",
        "    if palabra == 'ana':\n",
        "      palabra = 'anorexia'\n",
        "    if palabra == 'anorexic':\n",
        "      palabra = 'anorexia'\n",
        "    if palabra == 'mía':\n",
        "      palabra = 'bulimia'\n",
        "    if palabra == 'bulimic':\n",
        "      palabra = 'bulimia'\n",
        "    if palabra == 'mia':\n",
        "      palabra = 'bulimia'\n",
        "    if palabra == 'promia':\n",
        "      palabra = 'bulimia'\n",
        "    if palabra == 'bulymia':\n",
        "      palabra = 'bulimia'\n",
        "    if palabra == 'proana':\n",
        "      palabra = 'anorexia'\n",
        "    if palabra == 'styleana':\n",
        "      palabra = 'anorexia'\n",
        "    if palabra == 'miaana':\n",
        "      palabra = 'anorexia'\n",
        "    if palabra == 'anoreccia':\n",
        "      palabra = 'anorexia'\n",
        "    tweet.append(palabra)\n",
        "\n",
        "  return tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCe8_G_UQZSm"
      },
      "source": [
        "**Proceso de lematización**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vOJnJxVOf3n"
      },
      "outputs": [],
      "source": [
        "def lematizacionProc(tweet):\n",
        "    '''\n",
        "    La función lematizacionProc(tweet), tiene como objetivo lematizar cada tweet,\n",
        "    esto con el fin de obtener la forma de cada palabra.\n",
        "    El dato de entrada es una lista de strings y el dato de salida es\n",
        "    una lista de strings.\n",
        "    Dado que es un proceso de lematización se requiere contar con la oración\n",
        "    completa, para después proceder a unir cada lemma encontrado en una lista.\n",
        "    '''\n",
        "\n",
        "    frase = ' '.join(tweet)\n",
        "    doc = nlp(frase)\n",
        "\n",
        "    lemma = [word.lemma for sent in doc.sentences for word in sent.words]\n",
        "\n",
        "    return lemma\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhTlmfMjQeyD"
      },
      "source": [
        "**Bloque de funciones para el procesamiento de información**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7_rm5upGVdf"
      },
      "outputs": [],
      "source": [
        "def limpiezaAbsoluta(tweets):\n",
        "  '''\n",
        "    La función de limpiezaAbsoluta(tweets), tiene como objetivo hacer el llamdo\n",
        "    de todas las funciones diferentes para la limpieza de los datos provenientes\n",
        "    del archivos csv.\n",
        "    El dato de entrada es una lista de strings y el dato de salida es\n",
        "    una lista de strings.\n",
        "    Por cada proceso de limpieza se crea una variable diferente, con el fin de\n",
        "    evitar confusiones se crean diferentes variables para llevar acabo cada\n",
        "    llamda de las funciones.\n",
        "  '''\n",
        "\n",
        "  limpieza = []\n",
        "  jerga = []\n",
        "  lematizacion = []\n",
        "\n",
        "  #Expresiones regulares\n",
        "  for tweet in tweets:\n",
        "    limpieza.append(limpiar_texto(tweet))\n",
        "\n",
        "  #Jerga\n",
        "  for tweet in limpieza:\n",
        "    jerga.append(jerga_proceso(tweet))\n",
        "\n",
        "  #Lematización\n",
        "\n",
        "  lematizacion\n",
        "\n",
        "  for tweet in jerga:\n",
        "    lematizacion.append(lematizacionProc(tweet))\n",
        "\n",
        "  return lematizacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfhuUENI_uSL"
      },
      "outputs": [],
      "source": [
        "resultadoLimpio = limpiezaAbsoluta(tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L-AhvLkJw8N"
      },
      "outputs": [],
      "source": [
        "print(resultadoLimpio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMIFirizQPhP"
      },
      "source": [
        "# Atributos del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gEd4Snj2Rzo"
      },
      "source": [
        "**Bigramas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CRDlPf_61Ru"
      },
      "outputs": [],
      "source": [
        "def realizarBigramas(resultadoLimpio):\n",
        "  '''\n",
        "  La función de realizarBgramas(resultado_limpio) tiene como objetivo encontar\n",
        "  por cada tweet sus bigramas correspondintes\n",
        "  El dato de entrada es una lista de lista de strings y el dato de salida es\n",
        "  una lista de lista de strings, que contiene los bigramas correspondientes a\n",
        "  cada tweet.\n",
        "  '''\n",
        "  arrBigramas = []\n",
        "  for elem in resultadoLimpio:\n",
        "    bigrama = list(ngrams(elem,2))\n",
        "    arrBigramas.append([\" \".join(t) for t in bigrama])\n",
        "\n",
        "  return arrBigramas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbZ0O3oqMT4P"
      },
      "outputs": [],
      "source": [
        "def semejanzaBigramas(arrBigramas):\n",
        "  '''\n",
        "  La función de semejanzasBigramas(arrBigramas) tiene como objetivo definir los\n",
        "  vectires necesarios para procesar la semejanza entre cada tweet.\n",
        "  El dato de entrada es una lista de lista de strings y el dato de salida es\n",
        "  una lista de vectores.\n",
        "  Se crea una bagOfWords, la cual contiene los bigramas únicos encontrados entre\n",
        "  todos los tweets, después procede a comparar estos con cada uno de los bigramas\n",
        "  de cada tweet, agrega un 1 si ha sido igual y 0 de lo contrario.\n",
        "  '''\n",
        "  bagOfWords = [];\n",
        "\n",
        "  for elem in arrBigramas:\n",
        "    for bigrama in elem:\n",
        "      if bigrama not in bagOfWords:\n",
        "        bagOfWords.append(bigrama)\n",
        "\n",
        "  vectores = []\n",
        "\n",
        "  for elem in arrBigramas:\n",
        "    vector = []\n",
        "    for bigrama in bagOfWords:\n",
        "      if bigrama in elem:\n",
        "        vector.append(1) #Lo encontró\n",
        "      else:\n",
        "        vector.append(0)\n",
        "    vectores.append(vector)#No\n",
        "\n",
        "  return vectores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXU5Sgz2RiOM"
      },
      "outputs": [],
      "source": [
        "bigramas  = realizarBigramas(resultadoLimpio)\n",
        "\n",
        "arrVecSemejanza= semejanzaBigramas(bigramas)\n",
        "\n",
        "pairwise.cosine_similarity(arrVecSemejanza)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjMvtSdiWNBZ"
      },
      "source": [
        "De acuerdo al proceso realizado, la semejanza que se encontró entre tweets es mínima, por lo tanto, no se muestra más que el resultado 0. No obstante, esto también indica que el grado de similitud no es nulo, pero por ser pocas las palabras que se usan en cada tweet, se reduce el resultado de semejanza entre estos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J22AfRoeVPyr"
      },
      "source": [
        "**Frecuencia de palabras clave**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc1KdRCkXcWY"
      },
      "outputs": [],
      "source": [
        "def palabrasClaveFuente(datosLimpios):\n",
        "  '''\n",
        "  La función de palabrasClaveFunte(datosLimpio) tiene como objetivo contabilizar\n",
        "  las palabras clave relacionadas con el trastorno alimenticio de la anorexia.\n",
        "  El dato de entrada es una lista de lista de strings y el dato de salida es un\n",
        "  diccionario con las palabras clave y su conteo.\n",
        "  La definición de estas palabras proviene de la siguiente fuente:\n",
        "  http://ilitia.cua.uam.mx:8080/jspui/handle/123456789/884\n",
        "  '''\n",
        "  palabrasFrecuentes = {'yo':0, 'nosotros':0, 'social':0, 'amigo':0, 'amigos':0, 'amiga':0, 'humano':0, 'ansiedad':0, 'salud':0, 'sentir':0, 'tristeza':0, 'vida':0, 'sexual':0, 'casa':0, 'hogar':0}\n",
        "\n",
        "  for tweet in datosLimpios:\n",
        "    for palabra in tweet:\n",
        "      if palabra in palabrasFrecuentes:\n",
        "        palabrasFrecuentes[palabra] += 1;\n",
        "\n",
        "  return palabrasFrecuentes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh4DAK389l7b"
      },
      "outputs": [],
      "source": [
        "palabrasConteoFuente = palabrasClaveFuente(resultadoLimpio)\n",
        "print(palabrasConteoFuente)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJl8Y7ORBTl5"
      },
      "outputs": [],
      "source": [
        "def palabrasClave(datosLimpio):\n",
        "  '''\n",
        "  La función de palabrasClave(datosLimpio) tiene como objetivo contabilizar\n",
        "  las palabras clave relacionadas con el trastorno alimenticio de la anorexia,\n",
        "  de acuerdo a la información de los tweets ya preprocesados.\n",
        "  El dato de entrada es una lista de lista de strings y el dato de salida es un\n",
        "  diccionario con las palabras clave y su conteo\n",
        "  Se revisa cada tweet y las palabras encontradas con el fin de conocer aquellas\n",
        "  palabras con mayor recurrencia en los tweets.\n",
        "  '''\n",
        "\n",
        "  vocabulary = {}\n",
        "  for tweet in datosLimpio:\n",
        "    for palabra in tweet:\n",
        "      if palabra in vocabulary:\n",
        "        vocabulary[palabra] += 1;\n",
        "      else:\n",
        "        vocabulary[palabra] = 1;\n",
        "\n",
        "  relevantes  = sorted(vocabulary.items(), key=lambda item: item[1], reverse=True)[:10]\n",
        "  return relevantes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU321oViZjjr"
      },
      "outputs": [],
      "source": [
        "palabrasClaveDef = palabrasClave(resultadoLimpio)\n",
        "print(palabrasClaveDef)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvU8VoeHdCem"
      },
      "source": [
        "**TF-IDF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so91WxJSdBRk"
      },
      "outputs": [],
      "source": [
        "def calculoTDIDF(resultadoLimpio):\n",
        "  '''\n",
        "   La función de calculoTDIDF(resultadoLimpio) tiene como objetivo identificar\n",
        "   las palabras clave dentro de todo el conjunto de datos y el peso de estas.\n",
        "   El dato de entrada es una lista de lista de strings y el dato de salida es un\n",
        "   dataframe con las palabras clave y su peso correspondiente en cada tweet.\n",
        "   Se busca en tener una oracion con cada tweet y después se procede a realizar\n",
        "   el calculo de TF-IDF. Con el fin de tener información más legible se crea\n",
        "   un dataframe. Y se regresan unicamente las 10 palabras con mayor peso.\n",
        "  '''\n",
        "  #Obtener una oracion por cada tweet\n",
        "  oraciones = [' '.join(palabras) for palabras in resultadoLimpio]\n",
        "\n",
        "  vectorizer = TfidfVectorizer()\n",
        "\n",
        "  # Ajustar datos\n",
        "  matriz = vectorizer.fit_transform(oraciones)\n",
        "\n",
        "  # Matriz legible\n",
        "  df = pd.DataFrame(matriz.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "  tfidf_sums = df.sum(axis=0)\n",
        "\n",
        "  # Primeras 10 palabras  de TF-IDF\n",
        "  claves = tfidf_sums.sort_values(ascending=False).head(10)\n",
        "\n",
        "  return claves\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkdYI2A0bNjQ"
      },
      "outputs": [],
      "source": [
        "resTDIDF = calculoTDIDF(resultadoLimpio)\n",
        "print(resTDIDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYuU6UylfZ1A"
      },
      "source": [
        "# Ejecución clasificadores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw5p8K4dgGdb"
      },
      "outputs": [],
      "source": [
        "def vectorizacion(resultadoLimpio):\n",
        "  '''\n",
        "  La función de vectorizacion(resultadoLimpio) tiene como objetivo obtener el\n",
        "  valor vectorial por cada una de las palabaras encontradas en los tweets ya\n",
        "  preprocesados.\n",
        "  El dato de entrada es una lista de lista de strings y el dato de salida es un\n",
        "  objeto de tipo Word2Vec.\n",
        "  '''\n",
        "\n",
        "  model = Word2Vec(\n",
        "    sentences = resultadoLimpio,\n",
        "    vector_size = 70,\n",
        "    sg = 1,\n",
        "    window = 10,\n",
        "    min_count = 1,\n",
        "  )\n",
        "  return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ4xa8dNLTXD"
      },
      "outputs": [],
      "source": [
        "modeloVector = vectorizacion(resultadoLimpio)\n",
        "\n",
        "print(\"Palabras semjantes a anorexia, las 5 primeras\")\n",
        "words = modeloVector.wv.most_similar(\"anorexia\", topn=5)\n",
        "for word in words:\n",
        "  print(word)\n",
        "print()\n",
        "\n",
        "print(\"Palabras semjantes a ser, las 5 primeras\")\n",
        "words = modeloVector.wv.most_similar(\"ser\", topn=5)\n",
        "for word in words:\n",
        "  print(word)\n",
        "print()\n",
        "\n",
        "print(\"Palabras semjantes a comer, las 5 primeras\")\n",
        "words = modeloVector.wv.most_similar(\"comer\", topn=5)\n",
        "for word in words:\n",
        "  print(word)\n",
        "print()\n",
        "\n",
        "print(\"Palabras semjantes a más, las 5 primeras\")\n",
        "words = modeloVector.wv.most_similar(\"más\", topn=5)\n",
        "for word in words:\n",
        "  print(word)\n",
        "print()\n",
        "\n",
        "print(\"Palabras semjantes a querer, las 5 primeras\")\n",
        "words = modeloVector.wv.most_similar(\"querer\", topn=5)\n",
        "for word in words:\n",
        "  print(word)\n",
        "print()\n",
        "\n",
        "print(\"Palabras semjantes a poder, las 5 primeras\")\n",
        "words = modeloVector.wv.most_similar(\"poder\", topn=5)\n",
        "for word in words:\n",
        "  print(word)\n",
        "print()\n",
        "\n",
        "print(\"Palabras semjantes a si, las 5 primeras\")\n",
        "words = modeloVector.wv.most_similar(\"si\", topn=5)\n",
        "for word in words:\n",
        "  print(word)\n",
        "print()\n",
        "\n",
        "print(\"Palabras semjantes a bulimia, las 5 primeras\")\n",
        "words = modeloVector.wv.most_similar(\"bulimia\", topn=5)\n",
        "for word in words:\n",
        "  print(word)\n",
        "print()\n",
        "\n",
        "print(\"Palabras semjantes a día, las 5 primeras\")\n",
        "words = modeloVector.wv.most_similar(\"día\", topn=5)\n",
        "for word in words:\n",
        "  print(word)\n",
        "print()\n",
        "\n",
        "print(\"Palabras semjantes a yo, las 5 primeras\")\n",
        "words = modeloVector.wv.most_similar(\"yo\", topn=5)\n",
        "for word in words:\n",
        "  print(word)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwaXU7BEuDmU"
      },
      "outputs": [],
      "source": [
        "def transformacion(tweets):\n",
        "  '''\n",
        "  La función transformacion(tweets) tiene como objetivo que por cada palabra\n",
        "  encontrada en cada tweet, se tenga un valor vectorial.\n",
        "  El dato de entrada es una lista de strings y el dato de salida es un vector\n",
        "  por cada tweet.\n",
        "  Se busca que por cada tweet tener solo un vector que contenga todos los\n",
        "  valores de cada palabra.\n",
        "  '''\n",
        "  total = 0\n",
        "  for palabra in tweets:\n",
        "    total += modeloVector.wv[palabra]\n",
        "  return total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAvPSSF8jqGA"
      },
      "outputs": [],
      "source": [
        "def procesamiento(resultadoLimpio):\n",
        "  '''\n",
        "  La función procesamiento(resultadoLimpio) tiene como objetivo descomponer la\n",
        "  lista tweets.\n",
        "  El dato de entrada es una lista de lista de strings y el dato de salida es una\n",
        "  lista de vectores\n",
        "  Se busca que por cada tweet se obtenga su respectivo valor vectorial, respecto\n",
        "  a las palabras que contiene.\n",
        "  '''\n",
        "  numeroTweets = []\n",
        "  for tweet in resultadoLimpio:\n",
        "    numeroTweets.append(transformacion(tweet))\n",
        "  return numeroTweets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNsGd8t9kCrU"
      },
      "outputs": [],
      "source": [
        "valorVecTweet = procesamiento(resultadoLimpio)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qtyh-2_V_QMv"
      },
      "outputs": [],
      "source": [
        "print(len(valorVecTweet))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNNZ2FOWlGSC"
      },
      "source": [
        "**ÁRBOL DE DECISIÓN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m5fEKXiugEt"
      },
      "outputs": [],
      "source": [
        "#Variable independiente\n",
        "X = valorVecTweet\n",
        "#Variable dependiente\n",
        "y = clases\n",
        "\n",
        "\n",
        "#Se divide los datos que serán usado para el entrenamiento y los que serán usados para probar el modelo\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "#Se creo el objeto de Árbol de decisión\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
        "\n",
        "#Se entrena el objeto creado\n",
        "clf = clf.fit(X_train,y_train)\n",
        "\n",
        "#Se lleva a cabo la predicción con los datos seleccionados para la parte de prueba\n",
        "y_pred = clf.predict(X_test)\n",
        "#Se muestra la precisión del modelo\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "# Indica que tan preciso es en cuanto a las predicciones  positivas del modelo\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred, pos_label='anorexia'))#\n",
        "# Indica que tan preciso para identificar correctamenta los datos positivos\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred, pos_label='anorexia'))\n",
        "\n",
        "\n",
        "\n",
        "print( metrics.confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOkt-gU3Hewc"
      },
      "outputs": [],
      "source": [
        "dot_data = StringIO()\n",
        "export_graphviz(clf, out_file=dot_data,\n",
        "                filled=True, rounded=True,\n",
        "                special_characters=True,class_names=['0','1'])\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "Image(graph.create_png())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFYZ9bBGD6sU"
      },
      "source": [
        "**Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnXVARnED3wm"
      },
      "outputs": [],
      "source": [
        "#Variable independiente\n",
        "X = valorVecTweet\n",
        "#Variable dependiente\n",
        "y = clases\n",
        "\n",
        "\n",
        "#Se divide los datos que serán usado para el entrenamiento y los que serán usados para probar el modelo\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "#Se creo el objeto de Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, criterion=\"entropy\", max_depth=3)\n",
        "\n",
        "#Se entrena el objeto creado\n",
        "rf = rf.fit(X_train,y_train)\n",
        "\n",
        "#Se lleva a cabo la predicción con los datos seleccionados para la parte de prueba\n",
        "y_pred = rf.predict(X_test)\n",
        "#Se muestra la precisión del modelo\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "# Indica que tan preciso es en cuanto a las predicciones  positivas del modelo\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred, pos_label='anorexia'))#\n",
        "# Indica que tan preciso para identificar correctamenta los datos positivos\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred, pos_label='anorexia'))\n",
        "\n",
        "print( metrics.confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao75xTIPlsG7"
      },
      "source": [
        "**SVM Support Vector Machine**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEgg68lsIr4Q"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "#Se crea el objeto clasificador svm\n",
        "clf = svm.SVC(kernel='linear')\n",
        "\n",
        "#Se entrena el modelo con el 70% de los datos asignados\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# El modelo predice el valor para los datos designados a la parte de pruebas\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "#Se importa la libreías necesaria para calcular la precisión\n",
        "from sklearn import metrics\n",
        "\n",
        "# Indica que tan seguido el modelo está en lo correcto\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "# Indica que tan preciso es en cuanto a las predicciones  positivas del modelo\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred, pos_label='anorexia'))#\n",
        "# Indica que tan preciso para identificar correctamenta los datos positivos\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred, pos_label='anorexia'))\n",
        "\n",
        "print( metrics.confusion_matrix(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}